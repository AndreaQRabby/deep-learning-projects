{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class Classification Deep Learning Model for Glass Identification Using Keras\n",
    "### David Lowe\n",
    "### November 8, 2019\n",
    "\n",
    "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
    "\n",
    "SUMMARY: The purpose of this project is to construct a predictive model using various machine learning algorithms and to document the end-to-end steps using a template. The Glass Identification dataset is a multi-class classification situation where we are trying to predict one of several (more than two) possible outcomes.\n",
    "\n",
    "INTRODUCTION: The dataset involves predicting, from USA Forensic Science Service, six types of glass; defined in terms of their oxide content (i.e. Na, Fe, K, etc). The study of classification of types of glass was also partly motivated by criminological investigation. At the scene of the crime, the glass left can be used as evidenceâ€¦if it is correctly identified!\n",
    "\n",
    "ANALYSIS: The baseline performance of the model achieved an average accuracy score of 65.00%. After tuning the hyperparameters, the best model processed the training dataset with an accuracy of 70.00%. Furthermore, the final model processed the test dataset with an accuracy of 68.52%, which indicated that we might have a variance problem. We need to gather more data or apply regularization techniques in training to narrow the variance gap before deploying the model in production.\n",
    "\n",
    "CONCLUSION: For this dataset, the model built using Keras and TensorFlow achieved a satisfactory result and should be considered for future modeling activities.\n",
    "\n",
    "Dataset Used: Glass Identification Data Set\n",
    "\n",
    "Dataset ML Model: Multi-class classification with numerical attributes\n",
    "\n",
    "Dataset Reference: [https://archive.ics.uci.edu/ml/datasets/glass+identification]\n",
    "\n",
    "One potential source of performance benchmarks: [https://www.kaggle.com/uciml/glass]\n",
    "\n",
    "Any deep-learning modeling project genrally can be broken down into about six major tasks:\n",
    "0. Prepare Environment\n",
    "1. Load Data\n",
    "2. Define Model\n",
    "3. Fit and Evaluate Model\n",
    "4. Optimize Model\n",
    "5. Finalize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0. Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the warning message filter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed number for reproducible results\n",
    "seedNum = 888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load libraries and packages\n",
    "import random\n",
    "random.seed(seedNum)\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seedNum)\n",
    "import keras as K\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import smtplib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from email.message import EmailMessage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Begin the timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# Set up the verbose flag to print detailed messages for debugging (setting to True will activate)\n",
    "verbose = True\n",
    "tf.debugging.set_log_device_placement(verbose)\n",
    "\n",
    "# Set up the number of CPU cores available for multi-thread processing\n",
    "n_jobs = -1\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
    "notifyStatus = False\n",
    "\n",
    "# Set the number of folds for cross validation\n",
    "n_folds = 5\n",
    "\n",
    "# Set the flag for splitting the dataset\n",
    "splitDataset = True\n",
    "splitPercentage = 0.25\n",
    "\n",
    "# Set various default Keras modeling parameters\n",
    "default_kernel_init = K.initializers.RandomNormal(seed=seedNum)\n",
    "default_loss = 'categorical_crossentropy'\n",
    "default_optimizer = 'adam'\n",
    "default_epochs = 500\n",
    "default_batches = 8\n",
    "default_metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the email notification function\n",
    "def email_notify(msg_text):\n",
    "    sender = os.environ.get('MAIL_SENDER')\n",
    "    receiver = os.environ.get('MAIL_RECEIVER')\n",
    "    gateway = os.environ.get('SMTP_GATEWAY')\n",
    "    smtpuser = os.environ.get('SMTP_USERNAME')\n",
    "    password = os.environ.get('SMTP_PASSWORD')\n",
    "    if sender==None or receiver==None or gateway==None or smtpuser==None or password==None:\n",
    "        sys.exit(\"Incomplete email setup info. Script Processing Aborted!!!\")\n",
    "    msg = EmailMessage()\n",
    "    msg.set_content(msg_text)\n",
    "    msg['Subject'] = 'Notification from Keras Multi-Class Classification Script'\n",
    "    msg['From'] = sender\n",
    "    msg['To'] = receiver\n",
    "    server = smtplib.SMTP(gateway, 587)\n",
    "    server.starttls()\n",
    "    server.login(smtpuser, password)\n",
    "    server.send_message(msg)\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 0 Prepare Environment completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Load Data has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>targetVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.51596</td>\n",
       "      <td>12.79</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.62</td>\n",
       "      <td>72.97</td>\n",
       "      <td>0.64</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.51743</td>\n",
       "      <td>13.30</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.14</td>\n",
       "      <td>73.09</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.51756</td>\n",
       "      <td>13.15</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.05</td>\n",
       "      <td>73.24</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.51918</td>\n",
       "      <td>14.04</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.37</td>\n",
       "      <td>72.08</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.51755</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RI     Na    Mg    Al     Si     K    Ca   Ba    Fe  targetVar\n",
       "1   1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.00          1\n",
       "2   1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.00          1\n",
       "3   1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.00          1\n",
       "4   1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.00          1\n",
       "5   1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.00          1\n",
       "6   1.51596  12.79  3.61  1.62  72.97  0.64  8.07  0.0  0.26          1\n",
       "7   1.51743  13.30  3.60  1.14  73.09  0.58  8.17  0.0  0.00          1\n",
       "8   1.51756  13.15  3.61  1.05  73.24  0.57  8.24  0.0  0.00          1\n",
       "9   1.51918  14.04  3.58  1.37  72.08  0.56  8.30  0.0  0.00          1\n",
       "10  1.51755  13.00  3.60  1.36  72.99  0.57  8.40  0.0  0.11          1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "colNames = ['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe','targetVar']\n",
    "Xy_original = pd.read_csv(dataset_path, names=colNames, sep=',', header=None, index_col=0)\n",
    "\n",
    "# Take a peek at the dataframe after the import\n",
    "Xy_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 214 entries, 1 to 214\n",
      "Data columns (total 10 columns):\n",
      "RI           214 non-null float64\n",
      "Na           214 non-null float64\n",
      "Mg           214 non-null float64\n",
      "Al           214 non-null float64\n",
      "Si           214 non-null float64\n",
      "K            214 non-null float64\n",
      "Ca           214 non-null float64\n",
      "Ba           214 non-null float64\n",
      "Fe           214 non-null float64\n",
      "targetVar    214 non-null int64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 18.4 KB\n"
     ]
    }
   ],
   "source": [
    "Xy_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>targetVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.518365</td>\n",
       "      <td>13.407850</td>\n",
       "      <td>2.684533</td>\n",
       "      <td>1.444907</td>\n",
       "      <td>72.650935</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>8.956963</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>2.780374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.816604</td>\n",
       "      <td>1.442408</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>1.423153</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>2.103739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.511150</td>\n",
       "      <td>10.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>69.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.516523</td>\n",
       "      <td>12.907500</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.517680</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>72.790000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.519157</td>\n",
       "      <td>13.825000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>73.087500</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.172500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.533930</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>75.410000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>16.190000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RI          Na          Mg          Al          Si           K  \\\n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
       "mean     1.518365   13.407850    2.684533    1.444907   72.650935    0.497056   \n",
       "std      0.003037    0.816604    1.442408    0.499270    0.774546    0.652192   \n",
       "min      1.511150   10.730000    0.000000    0.290000   69.810000    0.000000   \n",
       "25%      1.516523   12.907500    2.115000    1.190000   72.280000    0.122500   \n",
       "50%      1.517680   13.300000    3.480000    1.360000   72.790000    0.555000   \n",
       "75%      1.519157   13.825000    3.600000    1.630000   73.087500    0.610000   \n",
       "max      1.533930   17.380000    4.490000    3.500000   75.410000    6.210000   \n",
       "\n",
       "               Ca          Ba          Fe   targetVar  \n",
       "count  214.000000  214.000000  214.000000  214.000000  \n",
       "mean     8.956963    0.175047    0.057009    2.780374  \n",
       "std      1.423153    0.497219    0.097439    2.103739  \n",
       "min      5.430000    0.000000    0.000000    1.000000  \n",
       "25%      8.240000    0.000000    0.000000    1.000000  \n",
       "50%      8.600000    0.000000    0.000000    2.000000  \n",
       "75%      9.172500    0.000000    0.100000    3.000000  \n",
       "max     16.190000    3.150000    0.510000    7.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI           0\n",
      "Na           0\n",
      "Mg           0\n",
      "Al           0\n",
      "Si           0\n",
      "K            0\n",
      "Ca           0\n",
      "Ba           0\n",
      "Fe           0\n",
      "targetVar    0\n",
      "dtype: int64\n",
      "Total number of NaN in the dataframe:  0\n"
     ]
    }
   ],
   "source": [
    "print(Xy_original.isnull().sum())\n",
    "print('Total number of NaN in the dataframe: ', Xy_original.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b) Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>targetVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.51596</td>\n",
       "      <td>12.79</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.62</td>\n",
       "      <td>72.97</td>\n",
       "      <td>0.64</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.51743</td>\n",
       "      <td>13.30</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.14</td>\n",
       "      <td>73.09</td>\n",
       "      <td>0.58</td>\n",
       "      <td>8.17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.51756</td>\n",
       "      <td>13.15</td>\n",
       "      <td>3.61</td>\n",
       "      <td>1.05</td>\n",
       "      <td>73.24</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.51918</td>\n",
       "      <td>14.04</td>\n",
       "      <td>3.58</td>\n",
       "      <td>1.37</td>\n",
       "      <td>72.08</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.51755</td>\n",
       "      <td>13.00</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         RI     Na    Mg    Al     Si     K    Ca   Ba    Fe  targetVar\n",
       "1   1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.00          1\n",
       "2   1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.00          1\n",
       "3   1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.00          1\n",
       "4   1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.00          1\n",
       "5   1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.00          1\n",
       "6   1.51596  12.79  3.61  1.62  72.97  0.64  8.07  0.0  0.26          1\n",
       "7   1.51743  13.30  3.60  1.14  73.09  0.58  8.17  0.0  0.00          1\n",
       "8   1.51756  13.15  3.61  1.05  73.24  0.57  8.24  0.0  0.00          1\n",
       "9   1.51918  14.04  3.58  1.37  72.08  0.56  8.30  0.0  0.00          1\n",
       "10  1.51755  13.00  3.60  1.36  72.99  0.57  8.40  0.0  0.11          1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize the class column to the name of targetVar if required\n",
    "# Xy_original = Xy_original.rename(columns={'old_name': 'targetVar'})\n",
    "\n",
    "# Dropping features\n",
    "# Xy_original.drop(columns=['attribute_name'], inplace=True)\n",
    "\n",
    "# Impute missing values\n",
    "# Xy_original['col_name'].fillna('someValue', inplace=True)\n",
    "# Xy_original['attribute_name'].fillna(value=Xy_original['attribute_name'].median(), inplace=True)\n",
    "\n",
    "# Convert columns from one data type to another\n",
    "# Xy_original.column_name = Xy_original.column_name.astype('int')\n",
    "# Xy_original.column_name = Xy_original.column_name.astype('category')\n",
    "\n",
    "# Convert features with Y/N levels into categorical feature of 1/0\n",
    "# def reClassSomecol(target):\n",
    "#     if (target == 'Y'): return 1\n",
    "#     else: return 0\n",
    "# Xy_original['targetVar'] = Xy_original['target'].apply(reClassSomecol)\n",
    "# Xy_original.drop(columns=['target'], inplace=True)\n",
    "\n",
    "# Take a peek at the dataframe after the cleaning\n",
    "Xy_original.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 214 entries, 1 to 214\n",
      "Data columns (total 10 columns):\n",
      "RI           214 non-null float64\n",
      "Na           214 non-null float64\n",
      "Mg           214 non-null float64\n",
      "Al           214 non-null float64\n",
      "Si           214 non-null float64\n",
      "K            214 non-null float64\n",
      "Ca           214 non-null float64\n",
      "Ba           214 non-null float64\n",
      "Fe           214 non-null float64\n",
      "targetVar    214 non-null int64\n",
      "dtypes: float64(9), int64(1)\n",
      "memory usage: 18.4 KB\n"
     ]
    }
   ],
   "source": [
    "Xy_original.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>targetVar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "      <td>214.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.518365</td>\n",
       "      <td>13.407850</td>\n",
       "      <td>2.684533</td>\n",
       "      <td>1.444907</td>\n",
       "      <td>72.650935</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>8.956963</td>\n",
       "      <td>0.175047</td>\n",
       "      <td>0.057009</td>\n",
       "      <td>2.780374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003037</td>\n",
       "      <td>0.816604</td>\n",
       "      <td>1.442408</td>\n",
       "      <td>0.499270</td>\n",
       "      <td>0.774546</td>\n",
       "      <td>0.652192</td>\n",
       "      <td>1.423153</td>\n",
       "      <td>0.497219</td>\n",
       "      <td>0.097439</td>\n",
       "      <td>2.103739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.511150</td>\n",
       "      <td>10.730000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>69.810000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.516523</td>\n",
       "      <td>12.907500</td>\n",
       "      <td>2.115000</td>\n",
       "      <td>1.190000</td>\n",
       "      <td>72.280000</td>\n",
       "      <td>0.122500</td>\n",
       "      <td>8.240000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.517680</td>\n",
       "      <td>13.300000</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>72.790000</td>\n",
       "      <td>0.555000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.519157</td>\n",
       "      <td>13.825000</td>\n",
       "      <td>3.600000</td>\n",
       "      <td>1.630000</td>\n",
       "      <td>73.087500</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>9.172500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.533930</td>\n",
       "      <td>17.380000</td>\n",
       "      <td>4.490000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>75.410000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>16.190000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RI          Na          Mg          Al          Si           K  \\\n",
       "count  214.000000  214.000000  214.000000  214.000000  214.000000  214.000000   \n",
       "mean     1.518365   13.407850    2.684533    1.444907   72.650935    0.497056   \n",
       "std      0.003037    0.816604    1.442408    0.499270    0.774546    0.652192   \n",
       "min      1.511150   10.730000    0.000000    0.290000   69.810000    0.000000   \n",
       "25%      1.516523   12.907500    2.115000    1.190000   72.280000    0.122500   \n",
       "50%      1.517680   13.300000    3.480000    1.360000   72.790000    0.555000   \n",
       "75%      1.519157   13.825000    3.600000    1.630000   73.087500    0.610000   \n",
       "max      1.533930   17.380000    4.490000    3.500000   75.410000    6.210000   \n",
       "\n",
       "               Ca          Ba          Fe   targetVar  \n",
       "count  214.000000  214.000000  214.000000  214.000000  \n",
       "mean     8.956963    0.175047    0.057009    2.780374  \n",
       "std      1.423153    0.497219    0.097439    2.103739  \n",
       "min      5.430000    0.000000    0.000000    1.000000  \n",
       "25%      8.240000    0.000000    0.000000    1.000000  \n",
       "50%      8.600000    0.000000    0.000000    2.000000  \n",
       "75%      9.172500    0.000000    0.100000    3.000000  \n",
       "max     16.190000    3.150000    0.510000    7.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xy_original.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RI           0\n",
      "Na           0\n",
      "Mg           0\n",
      "Al           0\n",
      "Si           0\n",
      "K            0\n",
      "Ca           0\n",
      "Ba           0\n",
      "Fe           0\n",
      "targetVar    0\n",
      "dtype: int64\n",
      "Total number of NaN in the dataframe:  0\n"
     ]
    }
   ],
   "source": [
    "print(Xy_original.isnull().sum())\n",
    "print('Total number of NaN in the dataframe: ', Xy_original.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.c) Feature Scaling and Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use variable totCol to hold the number of columns in the dataframe\n",
    "totCol = len(Xy_original.columns)\n",
    "\n",
    "# Set up variable totAttr for the total number of attribute columns\n",
    "totAttr = totCol-1\n",
    "\n",
    "# targetCol variable indicates the column location of the target/class variable\n",
    "# If the first column, set targetCol to 1. If the last column, set targetCol to totCol\n",
    "# If (targetCol <> 1) and (targetCol <> totCol), be aware when slicing up the dataframes for visualization\n",
    "targetCol = totCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xy_original.shape: (214, 10) X_original.shape: (214, 9) y_original.shape: (214,)\n"
     ]
    }
   ],
   "source": [
    "# We create attribute-only and target-only datasets (X_original and y_original)\n",
    "# for various visualization and cleaning/transformation operations\n",
    "\n",
    "if targetCol == totCol:\n",
    "    X_original = Xy_original.iloc[:,0:totAttr]\n",
    "    y_original = Xy_original.iloc[:,totAttr]\n",
    "else:\n",
    "    X_original = Xy_original.iloc[:,1:totCol]\n",
    "    y_original = Xy_original.iloc[:,0]\n",
    "\n",
    "print(\"Xy_original.shape: {} X_original.shape: {} y_original.shape: {}\".format(Xy_original.shape, X_original.shape, y_original.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the number of row and columns for visualization display. dispRow * dispCol should be >= totAttr\n",
    "dispCol = 4\n",
    "if totAttr % dispCol == 0 :\n",
    "    dispRow = totAttr // dispCol\n",
    "else :\n",
    "    dispRow = (totAttr // dispCol) + 1\n",
    "    \n",
    "# Set figure width to display the data visualization plots\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = dispCol*4\n",
    "fig_size[1] = dispRow*4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms for each attribute before pre-processing\n",
    "X_original.hist(layout=(dispRow,dispCol))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.87286765,  0.28495326,  1.25463857, ..., -0.14576634,\n",
       "        -0.35287683, -0.5864509 ],\n",
       "       [-0.24933347,  0.59181718,  0.63616803, ..., -0.79373376,\n",
       "        -0.35287683, -0.5864509 ],\n",
       "       [-0.72131806,  0.14993314,  0.60142249, ..., -0.82894938,\n",
       "        -0.35287683, -0.5864509 ],\n",
       "       ...,\n",
       "       [ 0.75404635,  1.16872135, -1.86551055, ..., -0.36410319,\n",
       "         2.95320036, -0.5864509 ],\n",
       "       [-0.61239854,  1.19327046, -1.86551055, ..., -0.33593069,\n",
       "         2.81208731, -0.5864509 ],\n",
       "       [-0.41436305,  1.00915211, -1.86551055, ..., -0.23732695,\n",
       "         3.01367739, -0.5864509 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply feature scaling and transformation\n",
    "\n",
    "# X_original['some_feature'] = preprocessing.scale(X_original['some_feature'])\n",
    "preprocessing.scale(X_original, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histograms for each attribute after pre-processing\n",
    "X_original.hist(layout=(dispRow,dispCol))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.d) Splitting Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Encode class values as integers and perform one-hot-encoding\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(y_original)\n",
    "y_transformed = encoder.transform(y_original)\n",
    "y_encoded = np_utils.to_categorical(y_transformed)\n",
    "print(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (160, 9) X_train.type: <class 'numpy.ndarray'>\n",
      "y_train.shape: (160, 6) y_train.type: <class 'numpy.ndarray'>\n",
      "X_test.shape: (54, 9) X_test.type: <class 'numpy.ndarray'>\n",
      "y_test.shape: (54, 6) y_test.type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_encoded = X_original.values\n",
    "if (splitDataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=splitPercentage, random_state=seedNum)\n",
    "else:\n",
    "    X_train, y_train = X_encoded, y_encoded\n",
    "    X_test, y_test = X_encoded, y_encoded\n",
    "print(\"X_train.shape: {} X_train.type: {}\".format(X_train.shape, type(X_train)))\n",
    "print(\"y_train.shape: {} y_train.type: {}\".format(y_train.shape, type(y_train)))\n",
    "print(\"X_test.shape: {} X_test.type: {}\".format(X_test.shape, type(X_test)))\n",
    "print(\"y_test.shape: {} y_test.type: {}\".format(y_test.shape, type(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 1 Load Data completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 2 Define Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model required for KerasClassifier\n",
    "def create_default_model():\n",
    "    default_model = K.models.Sequential()\n",
    "    default_model.add(Dense(24, input_dim=9, kernel_initializer=default_kernel_init, activation='relu'))\n",
    "    default_model.add(Dense(12, kernel_initializer=default_kernel_init, activation='relu'))\n",
    "    default_model.add(Dense(6, kernel_initializer=default_kernel_init, activation='softmax'))\n",
    "    default_model.compile(loss=default_loss, optimizer=default_optimizer, metrics=default_metrics)\n",
    "    return default_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Keras model\n",
    "cv_model = KerasClassifier(build_fn=create_default_model, epochs=default_epochs, batch_size=default_batches, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 2 Define Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 3 Fit and Evaluate Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Add in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarIsInitializedOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op LogicalNot in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Assert in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Fill in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op AssignVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op VarHandleOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_959 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_42044 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_43024 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op DestroyResourceOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_84109 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_85089 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_126174 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_127154 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_168239 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op RandomStandardNormal in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_169219 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op __inference_keras_scratch_graph_210304 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Generating results using the metrics of ['accuracy']\n",
      "All cross-Validate results: [0.78125 0.6875  0.5     0.53125 0.75   ]\n",
      "Baseline results [mean (std)]: 65.00% (11.42%)\n",
      "Total time for performing cross-validation of the default model: 0:01:47.456177\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "# Fit and evaluate the Keras model using 10-fold cross validation\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seedNum)\n",
    "results = cross_val_score(cv_model, X_train, y_train, cv=kfold)\n",
    "print('Generating results using the metrics of', default_metrics)\n",
    "print('All cross-Validate results:', results)\n",
    "print('Baseline results [mean (std)]: %.2f%% (%.2f%%)' % (results.mean()*100, results.std()*100))\n",
    "\n",
    "print('Total time for performing cross-validation of the default model:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 3 Fit and Evaluate Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Optimize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 4 Optimize Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model required for KerasClassifier\n",
    "def create_customized_model(optimizer, kernel_init):\n",
    "    customized_model = K.models.Sequential()\n",
    "    customized_model.add(Dense(24, input_dim=9, kernel_initializer=kernel_init, activation='relu'))\n",
    "    customized_model.add(Dense(12, kernel_initializer=kernel_init, activation='relu'))\n",
    "    customized_model.add(Dense(6, kernel_initializer=kernel_init, activation='softmax'))\n",
    "    customized_model.compile(loss=default_loss, optimizer=optimizer, metrics=default_metrics)\n",
    "    return customized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Executing op ReadVariableOp in device /job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed: 16.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_keras_scratch_graph_212355 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Best: 0.700000 using {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1500, 'batch_size': 16}\n",
      "0.643750 (0.098027) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 1500, 'batch_size': 32}\n",
      "0.693750 (0.107165) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1500, 'batch_size': 8}\n",
      "0.675000 (0.112847) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 500, 'batch_size': 16}\n",
      "0.637500 (0.127475) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.668750 (0.091856) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.643750 (0.134919) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.631250 (0.069597) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.656250 (0.120221) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.668750 (0.121192) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.637500 (0.064348) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1500, 'batch_size': 16}\n",
      "0.650000 (0.125623) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1000, 'batch_size': 8}\n",
      "0.687500 (0.121835) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1500, 'batch_size': 32}\n",
      "0.700000 (0.105697) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1500, 'batch_size': 16}\n",
      "0.656250 (0.094786) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 500, 'batch_size': 32}\n",
      "0.637500 (0.101934) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 500, 'batch_size': 32}\n",
      "0.656250 (0.086150) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1000, 'batch_size': 8}\n",
      "0.631250 (0.082443) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1500, 'batch_size': 32}\n",
      "0.656250 (0.094786) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1500, 'batch_size': 16}\n",
      "0.675000 (0.117925) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 500, 'batch_size': 8}\n",
      "0.687500 (0.065551) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1000, 'batch_size': 16}\n",
      "0.618750 (0.110750) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 1500, 'batch_size': 16}\n",
      "0.668750 (0.089704) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 500, 'batch_size': 16}\n",
      "0.681250 (0.108972) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 1000, 'batch_size': 8}\n",
      "0.593750 (0.108253) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 500, 'batch_size': 16}\n",
      "0.631250 (0.097628) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.687500 (0.086150) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1500, 'batch_size': 8}\n",
      "0.693750 (0.114223) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.681250 (0.115920) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 500, 'batch_size': 16}\n",
      "0.650000 (0.066732) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.681250 (0.101550) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 500, 'batch_size': 32}\n",
      "0.637500 (0.128999) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 500, 'batch_size': 8}\n",
      "0.637500 (0.064348) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 500, 'batch_size': 16}\n",
      "0.656250 (0.108253) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 500, 'batch_size': 32}\n",
      "0.650000 (0.072349) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 1500, 'batch_size': 8}\n",
      "0.643750 (0.096014) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 1500, 'batch_size': 16}\n",
      "0.662500 (0.084779) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 500, 'batch_size': 16}\n",
      "0.656250 (0.118585) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03d67d0>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 1000, 'batch_size': 32}\n",
      "0.625000 (0.059293) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.VarianceScaling object at 0x7f5ec03c22d0>, 'epochs': 1500, 'batch_size': 32}\n",
      "0.662500 (0.093541) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2990>, 'kernel_init': <keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, 'epochs': 500, 'batch_size': 8}\n",
      "0.656250 (0.106434) with: {'optimizer': <keras.optimizers.Adam object at 0x7f5ec03c2a10>, 'kernel_init': <keras.initializers.Orthogonal object at 0x7f5ec03c2090>, 'epochs': 500, 'batch_size': 16}\n",
      "Total time for performing grid-search of the best parameters: 0:16:51.060100\n"
     ]
    }
   ],
   "source": [
    "startTimeModule = datetime.now()\n",
    "\n",
    "# create model\n",
    "grid_model = KerasClassifier(build_fn=create_customized_model, verbose=0)\n",
    "\n",
    "# Perform grid search using different epochs, batch sizes, and optimizers\n",
    "optz_1 = K.optimizers.Adam(learning_rate=0.001)\n",
    "optz_2 = K.optimizers.Adam(learning_rate=0.005)\n",
    "optz_3 = K.optimizers.Adam(learning_rate=0.009)\n",
    "optimizer_grid = [optz_1, optz_2, optz_3]\n",
    "init_1 = K.initializers.RandomNormal(seed=seedNum)\n",
    "init_2 = K.initializers.glorot_normal(seed=seedNum)\n",
    "init_3 = K.initializers.Orthogonal(seed=seedNum)\n",
    "init_grid = [init_1, init_2, init_3]\n",
    "epoch_grid = [500, 1000, 1500]\n",
    "batch_grid = [8, 16, 32]\n",
    "param_grid = dict(optimizer=optimizer_grid, kernel_init=init_grid, epochs=epoch_grid, batch_size=batch_grid)\n",
    "# grid = GridSearchCV(estimator=grid_model, param_grid=param_grid, cv=n_folds, n_jobs=n_jobs, verbose=3)\n",
    "n_iter = int(len(optimizer_grid) * len(init_grid) * len(epoch_grid) * len(batch_grid) * 0.5)\n",
    "grid = RandomizedSearchCV(estimator=grid_model, param_distributions=param_grid, n_iter=n_iter, cv=n_folds, n_jobs=n_jobs, verbose=3)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "print('Total time for performing grid-search of the best parameters:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_optimizer = grid_result.best_params_[\"optimizer\"]\n",
    "best_kernel_init = grid_result.best_params_[\"kernel_init\"]\n",
    "best_epoch = grid_result.best_params_[\"epochs\"]\n",
    "best_batch = grid_result.best_params_[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forming the final model using: optimizer=<keras.optimizers.Adam object at 0x7f5ec03d67d0>, kernel=<keras.initializers.RandomNormal object at 0x7f5ec03c2a50>, epochs=1500, batch_size=16\n",
      "Epoch 1/1500\n",
      "Executing op __inference_keras_scratch_graph_291274 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 1.7667 - accuracy: 0.3500\n",
      "Epoch 2/1500\n",
      "160/160 [==============================] - 0s 207us/step - loss: 1.6129 - accuracy: 0.3562\n",
      "Epoch 3/1500\n",
      "160/160 [==============================] - 0s 202us/step - loss: 1.3876 - accuracy: 0.4125\n",
      "Epoch 4/1500\n",
      "160/160 [==============================] - 0s 248us/step - loss: 1.2192 - accuracy: 0.4938\n",
      "Epoch 5/1500\n",
      "160/160 [==============================] - 0s 203us/step - loss: 1.0859 - accuracy: 0.5562\n",
      "Epoch 6/1500\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.9976 - accuracy: 0.6062\n",
      "Epoch 7/1500\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.9318 - accuracy: 0.6313\n",
      "Epoch 8/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.8846 - accuracy: 0.6812\n",
      "Epoch 9/1500\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.8547 - accuracy: 0.6812\n",
      "Epoch 10/1500\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.8141 - accuracy: 0.7125\n",
      "Epoch 11/1500\n",
      "160/160 [==============================] - 0s 256us/step - loss: 0.7946 - accuracy: 0.6938\n",
      "Epoch 12/1500\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.7691 - accuracy: 0.7312\n",
      "Epoch 13/1500\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.7552 - accuracy: 0.7125\n",
      "Epoch 14/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.7357 - accuracy: 0.7125\n",
      "Epoch 15/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.7266 - accuracy: 0.7188\n",
      "Epoch 16/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.7156 - accuracy: 0.7312\n",
      "Epoch 17/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.7054 - accuracy: 0.7375\n",
      "Epoch 18/1500\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.6981 - accuracy: 0.7437\n",
      "Epoch 19/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.6971 - accuracy: 0.7188\n",
      "Epoch 20/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.6888 - accuracy: 0.7500\n",
      "Epoch 21/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.6769 - accuracy: 0.7437\n",
      "Epoch 22/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.6692 - accuracy: 0.7375\n",
      "Epoch 23/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.6593 - accuracy: 0.7500\n",
      "Epoch 24/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.6626 - accuracy: 0.7563\n",
      "Epoch 25/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.6536 - accuracy: 0.7563\n",
      "Epoch 26/1500\n",
      "160/160 [==============================] - 0s 249us/step - loss: 0.6462 - accuracy: 0.7437\n",
      "Epoch 27/1500\n",
      "160/160 [==============================] - 0s 262us/step - loss: 0.6448 - accuracy: 0.7500\n",
      "Epoch 28/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.6394 - accuracy: 0.7625\n",
      "Epoch 29/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.6357 - accuracy: 0.7625\n",
      "Epoch 30/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.6336 - accuracy: 0.7625\n",
      "Epoch 31/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.6327 - accuracy: 0.7437\n",
      "Epoch 32/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.6226 - accuracy: 0.7500\n",
      "Epoch 33/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.6223 - accuracy: 0.7563\n",
      "Epoch 34/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.6173 - accuracy: 0.7625\n",
      "Epoch 35/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.6155 - accuracy: 0.7563\n",
      "Epoch 36/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.6115 - accuracy: 0.7625\n",
      "Epoch 37/1500\n",
      "160/160 [==============================] - 0s 264us/step - loss: 0.6087 - accuracy: 0.7688\n",
      "Epoch 38/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.6055 - accuracy: 0.7625\n",
      "Epoch 39/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.6040 - accuracy: 0.7625\n",
      "Epoch 40/1500\n",
      "160/160 [==============================] - 0s 157us/step - loss: 0.5999 - accuracy: 0.7625\n",
      "Epoch 41/1500\n",
      "160/160 [==============================] - 0s 158us/step - loss: 0.5979 - accuracy: 0.7750\n",
      "Epoch 42/1500\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.5963 - accuracy: 0.7563\n",
      "Epoch 43/1500\n",
      "160/160 [==============================] - 0s 167us/step - loss: 0.5929 - accuracy: 0.7625\n",
      "Epoch 44/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.5899 - accuracy: 0.7750\n",
      "Epoch 45/1500\n",
      "160/160 [==============================] - 0s 166us/step - loss: 0.5859 - accuracy: 0.7812\n",
      "Epoch 46/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.5834 - accuracy: 0.7812\n",
      "Epoch 47/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5813 - accuracy: 0.7812\n",
      "Epoch 48/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.5827 - accuracy: 0.7875\n",
      "Epoch 49/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5747 - accuracy: 0.7937\n",
      "Epoch 50/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.5733 - accuracy: 0.7812\n",
      "Epoch 51/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.5723 - accuracy: 0.7750\n",
      "Epoch 52/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.5700 - accuracy: 0.7937\n",
      "Epoch 53/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.5740 - accuracy: 0.8000\n",
      "Epoch 54/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.5672 - accuracy: 0.8062\n",
      "Epoch 55/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.5622 - accuracy: 0.7937\n",
      "Epoch 56/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.5598 - accuracy: 0.8062\n",
      "Epoch 57/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5571 - accuracy: 0.8000\n",
      "Epoch 58/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.5580 - accuracy: 0.8062\n",
      "Epoch 59/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.5530 - accuracy: 0.8125\n",
      "Epoch 60/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.5517 - accuracy: 0.8000\n",
      "Epoch 61/1500\n",
      "160/160 [==============================] - 0s 242us/step - loss: 0.5486 - accuracy: 0.8062\n",
      "Epoch 62/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.5449 - accuracy: 0.8062\n",
      "Epoch 63/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.5466 - accuracy: 0.8125\n",
      "Epoch 64/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.5441 - accuracy: 0.8000\n",
      "Epoch 65/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5397 - accuracy: 0.8000\n",
      "Epoch 66/1500\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.5404 - accuracy: 0.8125\n",
      "Epoch 67/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.5363 - accuracy: 0.8188\n",
      "Epoch 68/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.5358 - accuracy: 0.8125\n",
      "Epoch 69/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.5328 - accuracy: 0.8125\n",
      "Epoch 70/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.5306 - accuracy: 0.8188\n",
      "Epoch 71/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.5299 - accuracy: 0.8188\n",
      "Epoch 72/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.5257 - accuracy: 0.8188\n",
      "Epoch 73/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.5269 - accuracy: 0.8188\n",
      "Epoch 74/1500\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.5266 - accuracy: 0.8000\n",
      "Epoch 75/1500\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.5211 - accuracy: 0.8125\n",
      "Epoch 76/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.5198 - accuracy: 0.8188\n",
      "Epoch 77/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.5170 - accuracy: 0.8188\n",
      "Epoch 78/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5173 - accuracy: 0.8062\n",
      "Epoch 79/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.5142 - accuracy: 0.8125\n",
      "Epoch 80/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.5116 - accuracy: 0.8188\n",
      "Epoch 81/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5105 - accuracy: 0.8188\n",
      "Epoch 82/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.5106 - accuracy: 0.8188\n",
      "Epoch 83/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.5127 - accuracy: 0.8125\n",
      "Epoch 84/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.5051 - accuracy: 0.8125\n",
      "Epoch 85/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.5048 - accuracy: 0.8250\n",
      "Epoch 86/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.5086 - accuracy: 0.8062\n",
      "Epoch 87/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.5049 - accuracy: 0.8188\n",
      "Epoch 88/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.5038 - accuracy: 0.8125\n",
      "Epoch 89/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.5022 - accuracy: 0.8188\n",
      "Epoch 90/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.5006 - accuracy: 0.8125\n",
      "Epoch 91/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.4958 - accuracy: 0.8125\n",
      "Epoch 92/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.4919 - accuracy: 0.8125\n",
      "Epoch 93/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.4909 - accuracy: 0.8250\n",
      "Epoch 94/1500\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.4917 - accuracy: 0.8188\n",
      "Epoch 95/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.4915 - accuracy: 0.8250\n",
      "Epoch 96/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.4893 - accuracy: 0.8125\n",
      "Epoch 97/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.4860 - accuracy: 0.8250\n",
      "Epoch 98/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.4851 - accuracy: 0.8250\n",
      "Epoch 99/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.4826 - accuracy: 0.8250\n",
      "Epoch 100/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.4831 - accuracy: 0.8250\n",
      "Epoch 101/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.4802 - accuracy: 0.8062\n",
      "Epoch 102/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.4785 - accuracy: 0.8125\n",
      "Epoch 103/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.4770 - accuracy: 0.8250\n",
      "Epoch 104/1500\n",
      "160/160 [==============================] - 0s 264us/step - loss: 0.4763 - accuracy: 0.8188\n",
      "Epoch 105/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.4736 - accuracy: 0.8250\n",
      "Epoch 106/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.4735 - accuracy: 0.8250\n",
      "Epoch 107/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.4723 - accuracy: 0.8250\n",
      "Epoch 108/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.4712 - accuracy: 0.8250\n",
      "Epoch 109/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.4689 - accuracy: 0.8188\n",
      "Epoch 110/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.4676 - accuracy: 0.8125\n",
      "Epoch 111/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.4654 - accuracy: 0.8250\n",
      "Epoch 112/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.4651 - accuracy: 0.8250\n",
      "Epoch 113/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.4651 - accuracy: 0.8062\n",
      "Epoch 114/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.4619 - accuracy: 0.8125\n",
      "Epoch 115/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.4606 - accuracy: 0.8188\n",
      "Epoch 116/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.4586 - accuracy: 0.8188\n",
      "Epoch 117/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.4564 - accuracy: 0.8250\n",
      "Epoch 118/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.4565 - accuracy: 0.8125\n",
      "Epoch 119/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.4522 - accuracy: 0.8250\n",
      "Epoch 120/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.4511 - accuracy: 0.8188\n",
      "Epoch 121/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.4499 - accuracy: 0.8250\n",
      "Epoch 122/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.4499 - accuracy: 0.8188\n",
      "Epoch 123/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.4491 - accuracy: 0.8125\n",
      "Epoch 124/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.4470 - accuracy: 0.8313\n",
      "Epoch 125/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.4443 - accuracy: 0.8188\n",
      "Epoch 126/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.4420 - accuracy: 0.8062\n",
      "Epoch 127/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.4406 - accuracy: 0.8250\n",
      "Epoch 128/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.4399 - accuracy: 0.8188\n",
      "Epoch 129/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.4374 - accuracy: 0.8250\n",
      "Epoch 130/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.4352 - accuracy: 0.8313\n",
      "Epoch 131/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.4327 - accuracy: 0.8375\n",
      "Epoch 132/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.4329 - accuracy: 0.8062\n",
      "Epoch 133/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.4310 - accuracy: 0.8313\n",
      "Epoch 134/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.4293 - accuracy: 0.8250\n",
      "Epoch 135/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.4271 - accuracy: 0.8375\n",
      "Epoch 136/1500\n",
      "160/160 [==============================] - 0s 251us/step - loss: 0.4284 - accuracy: 0.8375\n",
      "Epoch 137/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.4239 - accuracy: 0.8375\n",
      "Epoch 138/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.4256 - accuracy: 0.8188\n",
      "Epoch 139/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.4217 - accuracy: 0.8125\n",
      "Epoch 140/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.4201 - accuracy: 0.8375\n",
      "Epoch 141/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.4204 - accuracy: 0.8313\n",
      "Epoch 142/1500\n",
      "160/160 [==============================] - 0s 165us/step - loss: 0.4155 - accuracy: 0.8250\n",
      "Epoch 143/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.4159 - accuracy: 0.8313\n",
      "Epoch 144/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.4127 - accuracy: 0.8313\n",
      "Epoch 145/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.4125 - accuracy: 0.8250\n",
      "Epoch 146/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.4125 - accuracy: 0.8125\n",
      "Epoch 147/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.4083 - accuracy: 0.8250\n",
      "Epoch 148/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.4057 - accuracy: 0.8375\n",
      "Epoch 149/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.4042 - accuracy: 0.8375\n",
      "Epoch 150/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.4039 - accuracy: 0.8375\n",
      "Epoch 151/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.4064 - accuracy: 0.8313\n",
      "Epoch 152/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.4000 - accuracy: 0.8500\n",
      "Epoch 153/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.3997 - accuracy: 0.8375\n",
      "Epoch 154/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.3970 - accuracy: 0.8313\n",
      "Epoch 155/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.3943 - accuracy: 0.8375\n",
      "Epoch 156/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.3926 - accuracy: 0.8375\n",
      "Epoch 157/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.3905 - accuracy: 0.8313\n",
      "Epoch 158/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.3893 - accuracy: 0.8375\n",
      "Epoch 159/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.3895 - accuracy: 0.8313\n",
      "Epoch 160/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.3846 - accuracy: 0.8375\n",
      "Epoch 161/1500\n",
      "160/160 [==============================] - 0s 252us/step - loss: 0.3845 - accuracy: 0.8438\n",
      "Epoch 162/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.3847 - accuracy: 0.8438\n",
      "Epoch 163/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.3815 - accuracy: 0.8438\n",
      "Epoch 164/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.3791 - accuracy: 0.8438\n",
      "Epoch 165/1500\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.3768 - accuracy: 0.8438\n",
      "Epoch 166/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.3762 - accuracy: 0.8438\n",
      "Epoch 167/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.3750 - accuracy: 0.8375\n",
      "Epoch 168/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.3736 - accuracy: 0.8375\n",
      "Epoch 169/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.3701 - accuracy: 0.8500\n",
      "Epoch 170/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.3710 - accuracy: 0.8438\n",
      "Epoch 171/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.3670 - accuracy: 0.8500\n",
      "Epoch 172/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.3677 - accuracy: 0.8562\n",
      "Epoch 173/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.3627 - accuracy: 0.8500\n",
      "Epoch 174/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.3614 - accuracy: 0.8500\n",
      "Epoch 175/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.3593 - accuracy: 0.8500\n",
      "Epoch 176/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.3569 - accuracy: 0.8438\n",
      "Epoch 177/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.3603 - accuracy: 0.8562\n",
      "Epoch 178/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.3570 - accuracy: 0.8500\n",
      "Epoch 179/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.3539 - accuracy: 0.8500\n",
      "Epoch 180/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.3524 - accuracy: 0.8438\n",
      "Epoch 181/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.3504 - accuracy: 0.8438\n",
      "Epoch 182/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.3474 - accuracy: 0.8562\n",
      "Epoch 183/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.3456 - accuracy: 0.8500\n",
      "Epoch 184/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.3436 - accuracy: 0.8562\n",
      "Epoch 185/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.3412 - accuracy: 0.8500\n",
      "Epoch 186/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.3400 - accuracy: 0.8438\n",
      "Epoch 187/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.3367 - accuracy: 0.8500\n",
      "Epoch 188/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.3362 - accuracy: 0.8562\n",
      "Epoch 189/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.3341 - accuracy: 0.8625\n",
      "Epoch 190/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.3336 - accuracy: 0.8500\n",
      "Epoch 191/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.3309 - accuracy: 0.8562\n",
      "Epoch 192/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.3293 - accuracy: 0.8562\n",
      "Epoch 193/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.3281 - accuracy: 0.8500\n",
      "Epoch 194/1500\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.3250 - accuracy: 0.8562\n",
      "Epoch 195/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.3234 - accuracy: 0.8562\n",
      "Epoch 196/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.3243 - accuracy: 0.8687\n",
      "Epoch 197/1500\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.3211 - accuracy: 0.8625\n",
      "Epoch 198/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.3197 - accuracy: 0.8625\n",
      "Epoch 199/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.3161 - accuracy: 0.8625\n",
      "Epoch 200/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.3150 - accuracy: 0.8625\n",
      "Epoch 201/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.3157 - accuracy: 0.8625\n",
      "Epoch 202/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.3114 - accuracy: 0.8625\n",
      "Epoch 203/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.3093 - accuracy: 0.8562\n",
      "Epoch 204/1500\n",
      "160/160 [==============================] - 0s 167us/step - loss: 0.3065 - accuracy: 0.8625\n",
      "Epoch 205/1500\n",
      "160/160 [==============================] - 0s 168us/step - loss: 0.3044 - accuracy: 0.8625\n",
      "Epoch 206/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.3029 - accuracy: 0.8625\n",
      "Epoch 207/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.3032 - accuracy: 0.8625\n",
      "Epoch 208/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.3012 - accuracy: 0.8813\n",
      "Epoch 209/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.2975 - accuracy: 0.8750\n",
      "Epoch 210/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.2990 - accuracy: 0.8687\n",
      "Epoch 211/1500\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.2977 - accuracy: 0.8750\n",
      "Epoch 212/1500\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.2928 - accuracy: 0.8813\n",
      "Epoch 213/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.2919 - accuracy: 0.8687\n",
      "Epoch 214/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.2887 - accuracy: 0.8813\n",
      "Epoch 215/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.2891 - accuracy: 0.8750\n",
      "Epoch 216/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.2879 - accuracy: 0.8813\n",
      "Epoch 217/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.2849 - accuracy: 0.8750\n",
      "Epoch 218/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.2843 - accuracy: 0.8813\n",
      "Epoch 219/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.2827 - accuracy: 0.8875\n",
      "Epoch 220/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.2796 - accuracy: 0.8750\n",
      "Epoch 221/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.2795 - accuracy: 0.8750\n",
      "Epoch 222/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.2760 - accuracy: 0.8938\n",
      "Epoch 223/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.2749 - accuracy: 0.8875\n",
      "Epoch 224/1500\n",
      "160/160 [==============================] - 0s 269us/step - loss: 0.2746 - accuracy: 0.9000\n",
      "Epoch 225/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.2731 - accuracy: 0.8875\n",
      "Epoch 226/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.2754 - accuracy: 0.8938\n",
      "Epoch 227/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.2678 - accuracy: 0.8938\n",
      "Epoch 228/1500\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.2672 - accuracy: 0.8750\n",
      "Epoch 229/1500\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.2676 - accuracy: 0.8875\n",
      "Epoch 230/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.2645 - accuracy: 0.8750\n",
      "Epoch 231/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.2618 - accuracy: 0.8875\n",
      "Epoch 232/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.2614 - accuracy: 0.8813\n",
      "Epoch 233/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.2589 - accuracy: 0.8938\n",
      "Epoch 234/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.2576 - accuracy: 0.8938\n",
      "Epoch 235/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.2564 - accuracy: 0.8813\n",
      "Epoch 236/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.2545 - accuracy: 0.9000\n",
      "Epoch 237/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.2525 - accuracy: 0.9062\n",
      "Epoch 238/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.2515 - accuracy: 0.9125\n",
      "Epoch 239/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.2493 - accuracy: 0.9062\n",
      "Epoch 240/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.2479 - accuracy: 0.8938\n",
      "Epoch 241/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.2459 - accuracy: 0.9000\n",
      "Epoch 242/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.2447 - accuracy: 0.9000\n",
      "Epoch 243/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.2442 - accuracy: 0.9125\n",
      "Epoch 244/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.2434 - accuracy: 0.9000\n",
      "Epoch 245/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.2415 - accuracy: 0.9062\n",
      "Epoch 246/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.2393 - accuracy: 0.9125\n",
      "Epoch 247/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.2378 - accuracy: 0.9062\n",
      "Epoch 248/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.2354 - accuracy: 0.9062\n",
      "Epoch 249/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.2344 - accuracy: 0.9125\n",
      "Epoch 250/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.2349 - accuracy: 0.9062\n",
      "Epoch 251/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.2325 - accuracy: 0.9250\n",
      "Epoch 252/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.2297 - accuracy: 0.9250\n",
      "Epoch 253/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.2289 - accuracy: 0.9250\n",
      "Epoch 254/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.2275 - accuracy: 0.9187\n",
      "Epoch 255/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.2270 - accuracy: 0.9312\n",
      "Epoch 256/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.2258 - accuracy: 0.9187\n",
      "Epoch 257/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.2231 - accuracy: 0.9312\n",
      "Epoch 258/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.2229 - accuracy: 0.9250\n",
      "Epoch 259/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.2205 - accuracy: 0.9250\n",
      "Epoch 260/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.2207 - accuracy: 0.9312\n",
      "Epoch 261/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.2200 - accuracy: 0.9375\n",
      "Epoch 262/1500\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.2177 - accuracy: 0.9375\n",
      "Epoch 263/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.2167 - accuracy: 0.9312\n",
      "Epoch 264/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.2160 - accuracy: 0.9375\n",
      "Epoch 265/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.2165 - accuracy: 0.9375\n",
      "Epoch 266/1500\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.2157 - accuracy: 0.9187\n",
      "Epoch 267/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.2133 - accuracy: 0.9312\n",
      "Epoch 268/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.2131 - accuracy: 0.9375\n",
      "Epoch 269/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.2101 - accuracy: 0.9438\n",
      "Epoch 270/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.2095 - accuracy: 0.9312\n",
      "Epoch 271/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.2100 - accuracy: 0.9312\n",
      "Epoch 272/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.2076 - accuracy: 0.9250\n",
      "Epoch 273/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.2063 - accuracy: 0.9438\n",
      "Epoch 274/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.2076 - accuracy: 0.9312\n",
      "Epoch 275/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.2056 - accuracy: 0.9438\n",
      "Epoch 276/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.2046 - accuracy: 0.9438\n",
      "Epoch 277/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.2040 - accuracy: 0.9312\n",
      "Epoch 278/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.2013 - accuracy: 0.9312\n",
      "Epoch 279/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.2007 - accuracy: 0.9438\n",
      "Epoch 280/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.1996 - accuracy: 0.9375\n",
      "Epoch 281/1500\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.1990 - accuracy: 0.9250\n",
      "Epoch 282/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1985 - accuracy: 0.9312\n",
      "Epoch 283/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1975 - accuracy: 0.9438\n",
      "Epoch 284/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1971 - accuracy: 0.9375\n",
      "Epoch 285/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1950 - accuracy: 0.9375\n",
      "Epoch 286/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1937 - accuracy: 0.9375\n",
      "Epoch 287/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1954 - accuracy: 0.9375\n",
      "Epoch 288/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1946 - accuracy: 0.9375\n",
      "Epoch 289/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.2022 - accuracy: 0.9312\n",
      "Epoch 290/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.1917 - accuracy: 0.9375\n",
      "Epoch 291/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1914 - accuracy: 0.9375\n",
      "Epoch 292/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1906 - accuracy: 0.9375\n",
      "Epoch 293/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1909 - accuracy: 0.9312\n",
      "Epoch 294/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1871 - accuracy: 0.9312\n",
      "Epoch 295/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.1890 - accuracy: 0.9438\n",
      "Epoch 296/1500\n",
      "160/160 [==============================] - 0s 224us/step - loss: 0.1860 - accuracy: 0.9438\n",
      "Epoch 297/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1862 - accuracy: 0.9375\n",
      "Epoch 298/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.1851 - accuracy: 0.9375\n",
      "Epoch 299/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1846 - accuracy: 0.9375\n",
      "Epoch 300/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.1830 - accuracy: 0.9375\n",
      "Epoch 301/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1822 - accuracy: 0.9375\n",
      "Epoch 302/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1817 - accuracy: 0.9438\n",
      "Epoch 303/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.1804 - accuracy: 0.9438\n",
      "Epoch 304/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.1813 - accuracy: 0.9375\n",
      "Epoch 305/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1800 - accuracy: 0.9375\n",
      "Epoch 306/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1789 - accuracy: 0.9375\n",
      "Epoch 307/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1836 - accuracy: 0.9375\n",
      "Epoch 308/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1771 - accuracy: 0.9375\n",
      "Epoch 309/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1770 - accuracy: 0.9438\n",
      "Epoch 310/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.1768 - accuracy: 0.9438\n",
      "Epoch 311/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1764 - accuracy: 0.9375\n",
      "Epoch 312/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1765 - accuracy: 0.9375\n",
      "Epoch 313/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1765 - accuracy: 0.9312\n",
      "Epoch 314/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.1734 - accuracy: 0.9438\n",
      "Epoch 315/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1736 - accuracy: 0.9438\n",
      "Epoch 316/1500\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.1727 - accuracy: 0.9438\n",
      "Epoch 317/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1722 - accuracy: 0.9375\n",
      "Epoch 318/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1713 - accuracy: 0.9500\n",
      "Epoch 319/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1709 - accuracy: 0.9375\n",
      "Epoch 320/1500\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.1707 - accuracy: 0.9438\n",
      "Epoch 321/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.1697 - accuracy: 0.9500\n",
      "Epoch 322/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1695 - accuracy: 0.9375\n",
      "Epoch 323/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.1684 - accuracy: 0.9312\n",
      "Epoch 324/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1671 - accuracy: 0.9375\n",
      "Epoch 325/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1672 - accuracy: 0.9438\n",
      "Epoch 326/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1661 - accuracy: 0.9438\n",
      "Epoch 327/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.1654 - accuracy: 0.9500\n",
      "Epoch 328/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1655 - accuracy: 0.9500\n",
      "Epoch 329/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.1664 - accuracy: 0.9312\n",
      "Epoch 330/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1643 - accuracy: 0.9438\n",
      "Epoch 331/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.1639 - accuracy: 0.9438\n",
      "Epoch 332/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1650 - accuracy: 0.9438\n",
      "Epoch 333/1500\n",
      "160/160 [==============================] - 0s 244us/step - loss: 0.1610 - accuracy: 0.9438\n",
      "Epoch 334/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.1619 - accuracy: 0.9438\n",
      "Epoch 335/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.1604 - accuracy: 0.9500\n",
      "Epoch 336/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.1609 - accuracy: 0.9500\n",
      "Epoch 337/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1593 - accuracy: 0.9438\n",
      "Epoch 338/1500\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.1590 - accuracy: 0.9500\n",
      "Epoch 339/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.1599 - accuracy: 0.9438\n",
      "Epoch 340/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1592 - accuracy: 0.9375\n",
      "Epoch 341/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1623 - accuracy: 0.9500\n",
      "Epoch 342/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1584 - accuracy: 0.9500\n",
      "Epoch 343/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.1578 - accuracy: 0.9500\n",
      "Epoch 344/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.1583 - accuracy: 0.9500\n",
      "Epoch 345/1500\n",
      "160/160 [==============================] - 0s 226us/step - loss: 0.1579 - accuracy: 0.9375\n",
      "Epoch 346/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.1552 - accuracy: 0.9563\n",
      "Epoch 347/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.1548 - accuracy: 0.9563\n",
      "Epoch 348/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.1530 - accuracy: 0.9563\n",
      "Epoch 349/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.1538 - accuracy: 0.9563\n",
      "Epoch 350/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.1535 - accuracy: 0.9500\n",
      "Epoch 351/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1518 - accuracy: 0.9500\n",
      "Epoch 352/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.1526 - accuracy: 0.9563\n",
      "Epoch 353/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.1514 - accuracy: 0.9563\n",
      "Epoch 354/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1516 - accuracy: 0.9500\n",
      "Epoch 355/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1513 - accuracy: 0.9563\n",
      "Epoch 356/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1508 - accuracy: 0.9563\n",
      "Epoch 357/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.1513 - accuracy: 0.9563\n",
      "Epoch 358/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1494 - accuracy: 0.9563\n",
      "Epoch 359/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.1494 - accuracy: 0.9563\n",
      "Epoch 360/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.1480 - accuracy: 0.9563\n",
      "Epoch 361/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.1484 - accuracy: 0.9563\n",
      "Epoch 362/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1490 - accuracy: 0.9563\n",
      "Epoch 363/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.1491 - accuracy: 0.9563\n",
      "Epoch 364/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.1461 - accuracy: 0.9563\n",
      "Epoch 365/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1463 - accuracy: 0.9563\n",
      "Epoch 366/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1482 - accuracy: 0.9438\n",
      "Epoch 367/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1439 - accuracy: 0.9563\n",
      "Epoch 368/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1502 - accuracy: 0.9500\n",
      "Epoch 369/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.1437 - accuracy: 0.9563\n",
      "Epoch 370/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.1438 - accuracy: 0.9563\n",
      "Epoch 371/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1448 - accuracy: 0.9563\n",
      "Epoch 372/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.1438 - accuracy: 0.9563\n",
      "Epoch 373/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.1428 - accuracy: 0.9563\n",
      "Epoch 374/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.1420 - accuracy: 0.9563\n",
      "Epoch 375/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.1421 - accuracy: 0.9563\n",
      "Epoch 376/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.1414 - accuracy: 0.9625\n",
      "Epoch 377/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1408 - accuracy: 0.9563\n",
      "Epoch 378/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.1428 - accuracy: 0.9563\n",
      "Epoch 379/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.1441 - accuracy: 0.9563\n",
      "Epoch 380/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1398 - accuracy: 0.9563\n",
      "Epoch 381/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.1389 - accuracy: 0.9625\n",
      "Epoch 382/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1401 - accuracy: 0.9563\n",
      "Epoch 383/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1416 - accuracy: 0.9625\n",
      "Epoch 384/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.1388 - accuracy: 0.9563\n",
      "Epoch 385/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.1374 - accuracy: 0.9563\n",
      "Epoch 386/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.1376 - accuracy: 0.9563\n",
      "Epoch 387/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.1376 - accuracy: 0.9563\n",
      "Epoch 388/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.1368 - accuracy: 0.9563\n",
      "Epoch 389/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.1370 - accuracy: 0.9563\n",
      "Epoch 390/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1376 - accuracy: 0.9625\n",
      "Epoch 391/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1374 - accuracy: 0.9625\n",
      "Epoch 392/1500\n",
      "160/160 [==============================] - 0s 168us/step - loss: 0.1356 - accuracy: 0.9625\n",
      "Epoch 393/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.1345 - accuracy: 0.9625\n",
      "Epoch 394/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1333 - accuracy: 0.9625\n",
      "Epoch 395/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1335 - accuracy: 0.9625\n",
      "Epoch 396/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.1344 - accuracy: 0.9625\n",
      "Epoch 397/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1331 - accuracy: 0.9625\n",
      "Epoch 398/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.1323 - accuracy: 0.9688\n",
      "Epoch 399/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.1315 - accuracy: 0.9688\n",
      "Epoch 400/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.1313 - accuracy: 0.9625\n",
      "Epoch 401/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.1319 - accuracy: 0.9625\n",
      "Epoch 402/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.1313 - accuracy: 0.9625\n",
      "Epoch 403/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.1319 - accuracy: 0.9625\n",
      "Epoch 404/1500\n",
      "160/160 [==============================] - 0s 225us/step - loss: 0.1299 - accuracy: 0.9625\n",
      "Epoch 405/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.1307 - accuracy: 0.9625\n",
      "Epoch 406/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.1282 - accuracy: 0.9688\n",
      "Epoch 407/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.1295 - accuracy: 0.9625\n",
      "Epoch 408/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1288 - accuracy: 0.9625\n",
      "Epoch 409/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1284 - accuracy: 0.9625\n",
      "Epoch 410/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.1274 - accuracy: 0.9625\n",
      "Epoch 411/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1268 - accuracy: 0.9625\n",
      "Epoch 412/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.1272 - accuracy: 0.9625\n",
      "Epoch 413/1500\n",
      "160/160 [==============================] - 0s 160us/step - loss: 0.1270 - accuracy: 0.9625\n",
      "Epoch 414/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.1268 - accuracy: 0.9625\n",
      "Epoch 415/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.1265 - accuracy: 0.9625\n",
      "Epoch 416/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.1252 - accuracy: 0.9625\n",
      "Epoch 417/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1271 - accuracy: 0.9688\n",
      "Epoch 418/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1253 - accuracy: 0.9625\n",
      "Epoch 419/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.1251 - accuracy: 0.9625\n",
      "Epoch 420/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.1240 - accuracy: 0.9625\n",
      "Epoch 421/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1238 - accuracy: 0.9625\n",
      "Epoch 422/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.1228 - accuracy: 0.9688\n",
      "Epoch 423/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1236 - accuracy: 0.9688\n",
      "Epoch 424/1500\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.1220 - accuracy: 0.9688\n",
      "Epoch 425/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.1221 - accuracy: 0.9688\n",
      "Epoch 426/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.1215 - accuracy: 0.9625\n",
      "Epoch 427/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1215 - accuracy: 0.9688\n",
      "Epoch 428/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1220 - accuracy: 0.9625\n",
      "Epoch 429/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.1245 - accuracy: 0.9688\n",
      "Epoch 430/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1207 - accuracy: 0.9750\n",
      "Epoch 431/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.1215 - accuracy: 0.9688\n",
      "Epoch 432/1500\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.1216 - accuracy: 0.9688\n",
      "Epoch 433/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1200 - accuracy: 0.9750\n",
      "Epoch 434/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1183 - accuracy: 0.9625\n",
      "Epoch 435/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.1181 - accuracy: 0.9688\n",
      "Epoch 436/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1187 - accuracy: 0.9688\n",
      "Epoch 437/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.1187 - accuracy: 0.9688\n",
      "Epoch 438/1500\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.1170 - accuracy: 0.9688\n",
      "Epoch 439/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1176 - accuracy: 0.9625\n",
      "Epoch 440/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1162 - accuracy: 0.9688\n",
      "Epoch 441/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1165 - accuracy: 0.9625\n",
      "Epoch 442/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1151 - accuracy: 0.9750\n",
      "Epoch 443/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.1151 - accuracy: 0.9688\n",
      "Epoch 444/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.1155 - accuracy: 0.9750\n",
      "Epoch 445/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1148 - accuracy: 0.9750\n",
      "Epoch 446/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1138 - accuracy: 0.9750\n",
      "Epoch 447/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.1134 - accuracy: 0.9750\n",
      "Epoch 448/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.1136 - accuracy: 0.9750\n",
      "Epoch 449/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.1140 - accuracy: 0.9625\n",
      "Epoch 450/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.1166 - accuracy: 0.9625\n",
      "Epoch 451/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1135 - accuracy: 0.9750\n",
      "Epoch 452/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1119 - accuracy: 0.9688\n",
      "Epoch 453/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1116 - accuracy: 0.9750\n",
      "Epoch 454/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.1109 - accuracy: 0.9750\n",
      "Epoch 455/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.1107 - accuracy: 0.9750\n",
      "Epoch 456/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1102 - accuracy: 0.9750\n",
      "Epoch 457/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.1111 - accuracy: 0.9750\n",
      "Epoch 458/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1095 - accuracy: 0.9750\n",
      "Epoch 459/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1107 - accuracy: 0.9750\n",
      "Epoch 460/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.1108 - accuracy: 0.9750\n",
      "Epoch 461/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1096 - accuracy: 0.9750\n",
      "Epoch 462/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.1095 - accuracy: 0.9750\n",
      "Epoch 463/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.1076 - accuracy: 0.9750\n",
      "Epoch 464/1500\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.1075 - accuracy: 0.9750\n",
      "Epoch 465/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.1066 - accuracy: 0.9750\n",
      "Epoch 466/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1068 - accuracy: 0.9750\n",
      "Epoch 467/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.1087 - accuracy: 0.9750\n",
      "Epoch 468/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.1060 - accuracy: 0.9750\n",
      "Epoch 469/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1057 - accuracy: 0.9750\n",
      "Epoch 470/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1055 - accuracy: 0.9750\n",
      "Epoch 471/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.1050 - accuracy: 0.9750\n",
      "Epoch 472/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1042 - accuracy: 0.9750\n",
      "Epoch 473/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1052 - accuracy: 0.9750\n",
      "Epoch 474/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.1034 - accuracy: 0.9750\n",
      "Epoch 475/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.1046 - accuracy: 0.9812\n",
      "Epoch 476/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.1033 - accuracy: 0.9812\n",
      "Epoch 477/1500\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.1044 - accuracy: 0.9750\n",
      "Epoch 478/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.1054 - accuracy: 0.9750\n",
      "Epoch 479/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1054 - accuracy: 0.9750\n",
      "Epoch 480/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.1024 - accuracy: 0.9812\n",
      "Epoch 481/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.1028 - accuracy: 0.9750\n",
      "Epoch 482/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.1019 - accuracy: 0.9750\n",
      "Epoch 483/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.1013 - accuracy: 0.9750\n",
      "Epoch 484/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.1016 - accuracy: 0.9750\n",
      "Epoch 485/1500\n",
      "160/160 [==============================] - 0s 216us/step - loss: 0.1004 - accuracy: 0.9812\n",
      "Epoch 486/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.1002 - accuracy: 0.9750\n",
      "Epoch 487/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.1002 - accuracy: 0.9750\n",
      "Epoch 488/1500\n",
      "160/160 [==============================] - 0s 258us/step - loss: 0.0997 - accuracy: 0.9750\n",
      "Epoch 489/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.0995 - accuracy: 0.9812\n",
      "Epoch 490/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0990 - accuracy: 0.9750\n",
      "Epoch 491/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0988 - accuracy: 0.9750\n",
      "Epoch 492/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0983 - accuracy: 0.9750\n",
      "Epoch 493/1500\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.0983 - accuracy: 0.9750\n",
      "Epoch 494/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0986 - accuracy: 0.9812\n",
      "Epoch 495/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0999 - accuracy: 0.9750\n",
      "Epoch 496/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0993 - accuracy: 0.9750\n",
      "Epoch 497/1500\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.0979 - accuracy: 0.9812\n",
      "Epoch 498/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0992 - accuracy: 0.9750\n",
      "Epoch 499/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0960 - accuracy: 0.9812\n",
      "Epoch 500/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0975 - accuracy: 0.9812\n",
      "Epoch 501/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0971 - accuracy: 0.9750\n",
      "Epoch 502/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0971 - accuracy: 0.9750\n",
      "Epoch 503/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0960 - accuracy: 0.9750\n",
      "Epoch 504/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0955 - accuracy: 0.9750\n",
      "Epoch 505/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0937 - accuracy: 0.9812\n",
      "Epoch 506/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0971 - accuracy: 0.9812\n",
      "Epoch 507/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0949 - accuracy: 0.9812\n",
      "Epoch 508/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0939 - accuracy: 0.9750\n",
      "Epoch 509/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0932 - accuracy: 0.9750\n",
      "Epoch 510/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0944 - accuracy: 0.9812\n",
      "Epoch 511/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0933 - accuracy: 0.9812\n",
      "Epoch 512/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0924 - accuracy: 0.9812\n",
      "Epoch 513/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0937 - accuracy: 0.9750\n",
      "Epoch 514/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0914 - accuracy: 0.9750\n",
      "Epoch 515/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0931 - accuracy: 0.9750\n",
      "Epoch 516/1500\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.0916 - accuracy: 0.9812\n",
      "Epoch 517/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0920 - accuracy: 0.9812\n",
      "Epoch 518/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0929 - accuracy: 0.9812\n",
      "Epoch 519/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.0902 - accuracy: 0.9812\n",
      "Epoch 520/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0906 - accuracy: 0.9812\n",
      "Epoch 521/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0905 - accuracy: 0.9812\n",
      "Epoch 522/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0902 - accuracy: 0.9750\n",
      "Epoch 523/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0903 - accuracy: 0.9812\n",
      "Epoch 524/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0900 - accuracy: 0.9812\n",
      "Epoch 525/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0913 - accuracy: 0.9750\n",
      "Epoch 526/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0909 - accuracy: 0.9812\n",
      "Epoch 527/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0899 - accuracy: 0.9812\n",
      "Epoch 528/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0892 - accuracy: 0.9812\n",
      "Epoch 529/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0881 - accuracy: 0.9812\n",
      "Epoch 530/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0877 - accuracy: 0.9812\n",
      "Epoch 531/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0889 - accuracy: 0.9812\n",
      "Epoch 532/1500\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.0869 - accuracy: 0.9812\n",
      "Epoch 533/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0873 - accuracy: 0.9812\n",
      "Epoch 534/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0864 - accuracy: 0.9812\n",
      "Epoch 535/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0867 - accuracy: 0.9812\n",
      "Epoch 536/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0870 - accuracy: 0.9812\n",
      "Epoch 537/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0857 - accuracy: 0.9812\n",
      "Epoch 538/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0862 - accuracy: 0.9812\n",
      "Epoch 539/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0856 - accuracy: 0.9812\n",
      "Epoch 540/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0857 - accuracy: 0.9812\n",
      "Epoch 541/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0859 - accuracy: 0.9812\n",
      "Epoch 542/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0845 - accuracy: 0.9812\n",
      "Epoch 543/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.0849 - accuracy: 0.9812\n",
      "Epoch 544/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0857 - accuracy: 0.9812\n",
      "Epoch 545/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0842 - accuracy: 0.9812\n",
      "Epoch 546/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0842 - accuracy: 0.9812\n",
      "Epoch 547/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.0838 - accuracy: 0.9812\n",
      "Epoch 548/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.0829 - accuracy: 0.9812\n",
      "Epoch 549/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0833 - accuracy: 0.9812\n",
      "Epoch 550/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0844 - accuracy: 0.9812\n",
      "Epoch 551/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0834 - accuracy: 0.9812\n",
      "Epoch 552/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0843 - accuracy: 0.9812\n",
      "Epoch 553/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0830 - accuracy: 0.9812\n",
      "Epoch 554/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0815 - accuracy: 0.9812\n",
      "Epoch 555/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0825 - accuracy: 0.9812\n",
      "Epoch 556/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0817 - accuracy: 0.9812\n",
      "Epoch 557/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0817 - accuracy: 0.9812\n",
      "Epoch 558/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0810 - accuracy: 0.9812\n",
      "Epoch 559/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0813 - accuracy: 0.9812\n",
      "Epoch 560/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0806 - accuracy: 0.9812\n",
      "Epoch 561/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0804 - accuracy: 0.9812\n",
      "Epoch 562/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0800 - accuracy: 0.9812\n",
      "Epoch 563/1500\n",
      "160/160 [==============================] - 0s 222us/step - loss: 0.0799 - accuracy: 0.9812\n",
      "Epoch 564/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0789 - accuracy: 0.9812\n",
      "Epoch 565/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0793 - accuracy: 0.9812\n",
      "Epoch 566/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0788 - accuracy: 0.9812\n",
      "Epoch 567/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0801 - accuracy: 0.9812\n",
      "Epoch 568/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0795 - accuracy: 0.9812\n",
      "Epoch 569/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0784 - accuracy: 0.9812\n",
      "Epoch 570/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0805 - accuracy: 0.9812\n",
      "Epoch 571/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0798 - accuracy: 0.9812\n",
      "Epoch 572/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0797 - accuracy: 0.9812\n",
      "Epoch 573/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0774 - accuracy: 0.9812\n",
      "Epoch 574/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0773 - accuracy: 0.9812\n",
      "Epoch 575/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0765 - accuracy: 0.9812\n",
      "Epoch 576/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0776 - accuracy: 0.9812\n",
      "Epoch 577/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0771 - accuracy: 0.9812\n",
      "Epoch 578/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0757 - accuracy: 0.9812\n",
      "Epoch 579/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0769 - accuracy: 0.9812\n",
      "Epoch 580/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0759 - accuracy: 0.9812\n",
      "Epoch 581/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0751 - accuracy: 0.9812\n",
      "Epoch 582/1500\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.0749 - accuracy: 0.9812\n",
      "Epoch 583/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0755 - accuracy: 0.9812\n",
      "Epoch 584/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0750 - accuracy: 0.9812\n",
      "Epoch 585/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0748 - accuracy: 0.9812\n",
      "Epoch 586/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0756 - accuracy: 0.9812\n",
      "Epoch 587/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0764 - accuracy: 0.9812\n",
      "Epoch 588/1500\n",
      "160/160 [==============================] - 0s 262us/step - loss: 0.0734 - accuracy: 0.9812\n",
      "Epoch 589/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0744 - accuracy: 0.9812\n",
      "Epoch 590/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0741 - accuracy: 0.9812\n",
      "Epoch 591/1500\n",
      "160/160 [==============================] - 0s 211us/step - loss: 0.0727 - accuracy: 0.9812\n",
      "Epoch 592/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0736 - accuracy: 0.9812\n",
      "Epoch 593/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0728 - accuracy: 0.9812\n",
      "Epoch 594/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.0724 - accuracy: 0.9812\n",
      "Epoch 595/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0728 - accuracy: 0.9812\n",
      "Epoch 596/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0716 - accuracy: 0.9812\n",
      "Epoch 597/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0723 - accuracy: 0.9812\n",
      "Epoch 598/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0715 - accuracy: 0.9812\n",
      "Epoch 599/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0711 - accuracy: 0.9812\n",
      "Epoch 600/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0717 - accuracy: 0.9812\n",
      "Epoch 601/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0711 - accuracy: 0.9812\n",
      "Epoch 602/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0719 - accuracy: 0.9812\n",
      "Epoch 603/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0713 - accuracy: 0.9812\n",
      "Epoch 604/1500\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.0709 - accuracy: 0.9812\n",
      "Epoch 605/1500\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.0703 - accuracy: 0.9812\n",
      "Epoch 606/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0719 - accuracy: 0.9812\n",
      "Epoch 607/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0744 - accuracy: 0.9812\n",
      "Epoch 608/1500\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.0698 - accuracy: 0.9812\n",
      "Epoch 609/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0697 - accuracy: 0.9812\n",
      "Epoch 610/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0693 - accuracy: 0.9812\n",
      "Epoch 611/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0693 - accuracy: 0.9812\n",
      "Epoch 612/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0688 - accuracy: 0.9812\n",
      "Epoch 613/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0678 - accuracy: 0.9812\n",
      "Epoch 614/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0688 - accuracy: 0.9812\n",
      "Epoch 615/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0676 - accuracy: 0.9812\n",
      "Epoch 616/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0681 - accuracy: 0.9812\n",
      "Epoch 617/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0674 - accuracy: 0.9812\n",
      "Epoch 618/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0677 - accuracy: 0.9812\n",
      "Epoch 619/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0670 - accuracy: 0.9812\n",
      "Epoch 620/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0670 - accuracy: 0.9812\n",
      "Epoch 621/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0666 - accuracy: 0.9812\n",
      "Epoch 622/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0661 - accuracy: 0.9812\n",
      "Epoch 623/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0670 - accuracy: 0.9812\n",
      "Epoch 624/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0656 - accuracy: 0.9812\n",
      "Epoch 625/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0660 - accuracy: 0.9812\n",
      "Epoch 626/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0663 - accuracy: 0.9812\n",
      "Epoch 627/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0654 - accuracy: 0.9812\n",
      "Epoch 628/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0654 - accuracy: 0.9812\n",
      "Epoch 629/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0651 - accuracy: 0.9812\n",
      "Epoch 630/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0645 - accuracy: 0.9812\n",
      "Epoch 631/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0657 - accuracy: 0.9812\n",
      "Epoch 632/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0679 - accuracy: 0.9812\n",
      "Epoch 633/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0669 - accuracy: 0.9812\n",
      "Epoch 634/1500\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.0663 - accuracy: 0.9812\n",
      "Epoch 635/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0644 - accuracy: 0.9812\n",
      "Epoch 636/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0633 - accuracy: 0.9812\n",
      "Epoch 637/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0644 - accuracy: 0.9812\n",
      "Epoch 638/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0651 - accuracy: 0.9812\n",
      "Epoch 639/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0641 - accuracy: 0.9812\n",
      "Epoch 640/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0636 - accuracy: 0.9812\n",
      "Epoch 641/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0631 - accuracy: 0.9812\n",
      "Epoch 642/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0629 - accuracy: 0.9812\n",
      "Epoch 643/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0618 - accuracy: 0.9812\n",
      "Epoch 644/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0625 - accuracy: 0.9812\n",
      "Epoch 645/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0622 - accuracy: 0.9812\n",
      "Epoch 646/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0622 - accuracy: 0.9812\n",
      "Epoch 647/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0630 - accuracy: 0.9812\n",
      "Epoch 648/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0616 - accuracy: 0.9812\n",
      "Epoch 649/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0613 - accuracy: 0.9812\n",
      "Epoch 650/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0608 - accuracy: 0.9812\n",
      "Epoch 651/1500\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.0614 - accuracy: 0.9812\n",
      "Epoch 652/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0605 - accuracy: 0.9812\n",
      "Epoch 653/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0620 - accuracy: 0.9812\n",
      "Epoch 654/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0602 - accuracy: 0.9812\n",
      "Epoch 655/1500\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.0608 - accuracy: 0.9812\n",
      "Epoch 656/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0602 - accuracy: 0.9812\n",
      "Epoch 657/1500\n",
      "160/160 [==============================] - 0s 223us/step - loss: 0.0622 - accuracy: 0.9812\n",
      "Epoch 658/1500\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.0591 - accuracy: 0.9812\n",
      "Epoch 659/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0598 - accuracy: 0.9812\n",
      "Epoch 660/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0597 - accuracy: 0.9812\n",
      "Epoch 661/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0586 - accuracy: 0.9812\n",
      "Epoch 662/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0581 - accuracy: 0.9812\n",
      "Epoch 663/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0602 - accuracy: 0.9812\n",
      "Epoch 664/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0576 - accuracy: 0.9812\n",
      "Epoch 665/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0586 - accuracy: 0.9812\n",
      "Epoch 666/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0582 - accuracy: 0.9812\n",
      "Epoch 667/1500\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.0584 - accuracy: 0.9812\n",
      "Epoch 668/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0579 - accuracy: 0.9812\n",
      "Epoch 669/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0579 - accuracy: 0.9812\n",
      "Epoch 670/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0586 - accuracy: 0.9812\n",
      "Epoch 671/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0575 - accuracy: 0.9812\n",
      "Epoch 672/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0600 - accuracy: 0.9812\n",
      "Epoch 673/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0572 - accuracy: 0.9812\n",
      "Epoch 674/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0563 - accuracy: 0.9812\n",
      "Epoch 675/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0571 - accuracy: 0.9812\n",
      "Epoch 676/1500\n",
      "160/160 [==============================] - 0s 149us/step - loss: 0.0562 - accuracy: 0.9812\n",
      "Epoch 677/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0561 - accuracy: 0.9812\n",
      "Epoch 678/1500\n",
      "160/160 [==============================] - 0s 157us/step - loss: 0.0560 - accuracy: 0.9812\n",
      "Epoch 679/1500\n",
      "160/160 [==============================] - 0s 161us/step - loss: 0.0560 - accuracy: 0.9812\n",
      "Epoch 680/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0556 - accuracy: 0.9812\n",
      "Epoch 681/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0560 - accuracy: 0.9812\n",
      "Epoch 682/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0555 - accuracy: 0.9812\n",
      "Epoch 683/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0548 - accuracy: 0.9812\n",
      "Epoch 684/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0547 - accuracy: 0.9812\n",
      "Epoch 685/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0545 - accuracy: 0.9812\n",
      "Epoch 686/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0543 - accuracy: 0.9812\n",
      "Epoch 687/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0583 - accuracy: 0.9812\n",
      "Epoch 688/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0548 - accuracy: 0.9812\n",
      "Epoch 689/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0544 - accuracy: 0.9812\n",
      "Epoch 690/1500\n",
      "160/160 [==============================] - 0s 165us/step - loss: 0.0538 - accuracy: 0.9812\n",
      "Epoch 691/1500\n",
      "160/160 [==============================] - 0s 167us/step - loss: 0.0554 - accuracy: 0.9812\n",
      "Epoch 692/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0543 - accuracy: 0.9812\n",
      "Epoch 693/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0529 - accuracy: 0.9812\n",
      "Epoch 694/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0541 - accuracy: 0.9812\n",
      "Epoch 695/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0521 - accuracy: 0.9812\n",
      "Epoch 696/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0537 - accuracy: 0.9812\n",
      "Epoch 697/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0540 - accuracy: 0.9812\n",
      "Epoch 698/1500\n",
      "160/160 [==============================] - 0s 159us/step - loss: 0.0529 - accuracy: 0.9812\n",
      "Epoch 699/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0525 - accuracy: 0.9812\n",
      "Epoch 700/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0521 - accuracy: 0.9812\n",
      "Epoch 701/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0515 - accuracy: 0.9812\n",
      "Epoch 702/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0517 - accuracy: 0.9812\n",
      "Epoch 703/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0509 - accuracy: 0.9812\n",
      "Epoch 704/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0512 - accuracy: 0.9812\n",
      "Epoch 705/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0509 - accuracy: 0.9812\n",
      "Epoch 706/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0512 - accuracy: 0.9812\n",
      "Epoch 707/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0506 - accuracy: 0.9812\n",
      "Epoch 708/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0506 - accuracy: 0.9812\n",
      "Epoch 709/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0501 - accuracy: 0.9812\n",
      "Epoch 710/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0501 - accuracy: 0.9812\n",
      "Epoch 711/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0500 - accuracy: 0.9812\n",
      "Epoch 712/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0507 - accuracy: 0.9812\n",
      "Epoch 713/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0503 - accuracy: 0.9812\n",
      "Epoch 714/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0496 - accuracy: 0.9812\n",
      "Epoch 715/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0492 - accuracy: 0.9812\n",
      "Epoch 716/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0498 - accuracy: 0.9812\n",
      "Epoch 717/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0514 - accuracy: 0.9875\n",
      "Epoch 718/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0489 - accuracy: 0.9812\n",
      "Epoch 719/1500\n",
      "160/160 [==============================] - 0s 288us/step - loss: 0.0525 - accuracy: 0.9812\n",
      "Epoch 720/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0480 - accuracy: 0.9812\n",
      "Epoch 721/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0495 - accuracy: 0.9812\n",
      "Epoch 722/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0488 - accuracy: 0.9812\n",
      "Epoch 723/1500\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.0482 - accuracy: 0.9812\n",
      "Epoch 724/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0482 - accuracy: 0.9812\n",
      "Epoch 725/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0502 - accuracy: 0.9812\n",
      "Epoch 726/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0481 - accuracy: 0.9812\n",
      "Epoch 727/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0490 - accuracy: 0.9875\n",
      "Epoch 728/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0481 - accuracy: 0.9875\n",
      "Epoch 729/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0485 - accuracy: 0.9812\n",
      "Epoch 730/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0483 - accuracy: 0.9812\n",
      "Epoch 731/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0466 - accuracy: 0.9812\n",
      "Epoch 732/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0485 - accuracy: 0.9875\n",
      "Epoch 733/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0473 - accuracy: 0.9875\n",
      "Epoch 734/1500\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.0467 - accuracy: 0.9875\n",
      "Epoch 735/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0464 - accuracy: 0.9875\n",
      "Epoch 736/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0461 - accuracy: 0.9875\n",
      "Epoch 737/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0459 - accuracy: 0.9875\n",
      "Epoch 738/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0457 - accuracy: 0.9875\n",
      "Epoch 739/1500\n",
      "160/160 [==============================] - 0s 241us/step - loss: 0.0454 - accuracy: 0.9875\n",
      "Epoch 740/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0464 - accuracy: 0.9875\n",
      "Epoch 741/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0451 - accuracy: 0.9875\n",
      "Epoch 742/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0470 - accuracy: 0.9812\n",
      "Epoch 743/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0453 - accuracy: 0.9812\n",
      "Epoch 744/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0457 - accuracy: 0.9875\n",
      "Epoch 745/1500\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.0449 - accuracy: 0.9875\n",
      "Epoch 746/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0444 - accuracy: 0.9875\n",
      "Epoch 747/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0444 - accuracy: 0.9875\n",
      "Epoch 748/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0443 - accuracy: 0.9875\n",
      "Epoch 749/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0449 - accuracy: 0.9875\n",
      "Epoch 750/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0449 - accuracy: 0.9812\n",
      "Epoch 751/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0438 - accuracy: 0.9812\n",
      "Epoch 752/1500\n",
      "160/160 [==============================] - 0s 268us/step - loss: 0.0437 - accuracy: 0.9875\n",
      "Epoch 753/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0451 - accuracy: 0.9875\n",
      "Epoch 754/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0427 - accuracy: 0.9875\n",
      "Epoch 755/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0443 - accuracy: 0.9875\n",
      "Epoch 756/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0430 - accuracy: 0.9875\n",
      "Epoch 757/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0435 - accuracy: 0.9875\n",
      "Epoch 758/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0425 - accuracy: 0.9875\n",
      "Epoch 759/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0436 - accuracy: 0.9812\n",
      "Epoch 760/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0426 - accuracy: 0.9875\n",
      "Epoch 761/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0428 - accuracy: 0.9875\n",
      "Epoch 762/1500\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.0427 - accuracy: 0.9875\n",
      "Epoch 763/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0434 - accuracy: 0.9875\n",
      "Epoch 764/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0427 - accuracy: 0.9875\n",
      "Epoch 765/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0417 - accuracy: 0.9875\n",
      "Epoch 766/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0421 - accuracy: 0.9875\n",
      "Epoch 767/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0419 - accuracy: 0.9875\n",
      "Epoch 768/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0430 - accuracy: 0.9875\n",
      "Epoch 769/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.0416 - accuracy: 0.9875\n",
      "Epoch 770/1500\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.0414 - accuracy: 0.9875\n",
      "Epoch 771/1500\n",
      "160/160 [==============================] - 0s 210us/step - loss: 0.0411 - accuracy: 0.9875\n",
      "Epoch 772/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0414 - accuracy: 0.9875\n",
      "Epoch 773/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0404 - accuracy: 0.9875\n",
      "Epoch 774/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0407 - accuracy: 0.9875\n",
      "Epoch 775/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0414 - accuracy: 0.9875\n",
      "Epoch 776/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0401 - accuracy: 0.9875\n",
      "Epoch 777/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0403 - accuracy: 0.9875\n",
      "Epoch 778/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0405 - accuracy: 0.9875\n",
      "Epoch 779/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0394 - accuracy: 0.9875\n",
      "Epoch 780/1500\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.0403 - accuracy: 0.9875\n",
      "Epoch 781/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0428 - accuracy: 0.9875\n",
      "Epoch 782/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0402 - accuracy: 0.9875\n",
      "Epoch 783/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0397 - accuracy: 0.9875\n",
      "Epoch 784/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0421 - accuracy: 0.9875\n",
      "Epoch 785/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0391 - accuracy: 0.9875\n",
      "Epoch 786/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0384 - accuracy: 0.9875\n",
      "Epoch 787/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0394 - accuracy: 0.9875\n",
      "Epoch 788/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0393 - accuracy: 0.9875\n",
      "Epoch 789/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0387 - accuracy: 0.9875\n",
      "Epoch 790/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0381 - accuracy: 0.9875\n",
      "Epoch 791/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0384 - accuracy: 0.9875\n",
      "Epoch 792/1500\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.0380 - accuracy: 0.9875\n",
      "Epoch 793/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.0378 - accuracy: 0.9875\n",
      "Epoch 794/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0383 - accuracy: 0.9875\n",
      "Epoch 795/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0379 - accuracy: 0.9875\n",
      "Epoch 796/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0379 - accuracy: 0.9875\n",
      "Epoch 797/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0395 - accuracy: 0.9875\n",
      "Epoch 798/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0378 - accuracy: 0.9875\n",
      "Epoch 799/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0376 - accuracy: 0.9875\n",
      "Epoch 800/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0381 - accuracy: 0.9875\n",
      "Epoch 801/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0366 - accuracy: 0.9875\n",
      "Epoch 802/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0363 - accuracy: 0.9875\n",
      "Epoch 803/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0365 - accuracy: 0.9875\n",
      "Epoch 804/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0366 - accuracy: 0.9875\n",
      "Epoch 805/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0369 - accuracy: 0.9875\n",
      "Epoch 806/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0375 - accuracy: 0.9937\n",
      "Epoch 807/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0352 - accuracy: 0.9937\n",
      "Epoch 808/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0379 - accuracy: 0.9875\n",
      "Epoch 809/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0356 - accuracy: 0.9875\n",
      "Epoch 810/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0357 - accuracy: 0.9937\n",
      "Epoch 811/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0357 - accuracy: 0.9875\n",
      "Epoch 812/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0358 - accuracy: 0.9875\n",
      "Epoch 813/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0350 - accuracy: 0.9875\n",
      "Epoch 814/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0351 - accuracy: 0.9875\n",
      "Epoch 815/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0362 - accuracy: 0.9937\n",
      "Epoch 816/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0346 - accuracy: 0.9875\n",
      "Epoch 817/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0359 - accuracy: 0.9875\n",
      "Epoch 818/1500\n",
      "160/160 [==============================] - 0s 269us/step - loss: 0.0348 - accuracy: 0.9937\n",
      "Epoch 819/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0353 - accuracy: 0.9875\n",
      "Epoch 820/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0345 - accuracy: 0.9875\n",
      "Epoch 821/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0344 - accuracy: 0.9875\n",
      "Epoch 822/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0334 - accuracy: 0.9937\n",
      "Epoch 823/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0334 - accuracy: 0.9875\n",
      "Epoch 824/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0342 - accuracy: 0.9875\n",
      "Epoch 825/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0334 - accuracy: 0.9937\n",
      "Epoch 826/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0336 - accuracy: 0.9875\n",
      "Epoch 827/1500\n",
      "160/160 [==============================] - 0s 246us/step - loss: 0.0332 - accuracy: 0.9875\n",
      "Epoch 828/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0336 - accuracy: 0.9875\n",
      "Epoch 829/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0329 - accuracy: 0.9937\n",
      "Epoch 830/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0336 - accuracy: 0.9875\n",
      "Epoch 831/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0349 - accuracy: 0.9875\n",
      "Epoch 832/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0349 - accuracy: 0.9875\n",
      "Epoch 833/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0327 - accuracy: 0.9937\n",
      "Epoch 834/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0326 - accuracy: 0.9875\n",
      "Epoch 835/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0332 - accuracy: 0.9875\n",
      "Epoch 836/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.0322 - accuracy: 0.9937\n",
      "Epoch 837/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.0324 - accuracy: 0.9937\n",
      "Epoch 838/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0317 - accuracy: 0.9937\n",
      "Epoch 839/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0318 - accuracy: 0.9937\n",
      "Epoch 840/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0314 - accuracy: 0.9937\n",
      "Epoch 841/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0315 - accuracy: 0.9937\n",
      "Epoch 842/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0314 - accuracy: 0.9937\n",
      "Epoch 843/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0311 - accuracy: 0.9875\n",
      "Epoch 844/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0309 - accuracy: 0.9937\n",
      "Epoch 845/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0307 - accuracy: 0.9937\n",
      "Epoch 846/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0324 - accuracy: 0.9875\n",
      "Epoch 847/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0306 - accuracy: 0.9937\n",
      "Epoch 848/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0309 - accuracy: 0.9937\n",
      "Epoch 849/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.0322 - accuracy: 0.9875\n",
      "Epoch 850/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0307 - accuracy: 0.9937\n",
      "Epoch 851/1500\n",
      "160/160 [==============================] - 0s 269us/step - loss: 0.0307 - accuracy: 0.9937\n",
      "Epoch 852/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0306 - accuracy: 0.9937\n",
      "Epoch 853/1500\n",
      "160/160 [==============================] - 0s 255us/step - loss: 0.0305 - accuracy: 0.9937\n",
      "Epoch 854/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0301 - accuracy: 0.9937\n",
      "Epoch 855/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0297 - accuracy: 0.9937\n",
      "Epoch 856/1500\n",
      "160/160 [==============================] - 0s 166us/step - loss: 0.0300 - accuracy: 0.9937\n",
      "Epoch 857/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0296 - accuracy: 0.9937\n",
      "Epoch 858/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0293 - accuracy: 0.9937\n",
      "Epoch 859/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.0296 - accuracy: 0.9937\n",
      "Epoch 860/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0296 - accuracy: 0.9937\n",
      "Epoch 861/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0292 - accuracy: 0.9937\n",
      "Epoch 862/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0295 - accuracy: 0.9937\n",
      "Epoch 863/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0290 - accuracy: 0.9937\n",
      "Epoch 864/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0302 - accuracy: 0.9937\n",
      "Epoch 865/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0290 - accuracy: 0.9937\n",
      "Epoch 866/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0291 - accuracy: 0.9937\n",
      "Epoch 867/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0302 - accuracy: 0.9875\n",
      "Epoch 868/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0295 - accuracy: 0.9875\n",
      "Epoch 869/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0287 - accuracy: 0.9937\n",
      "Epoch 870/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0293 - accuracy: 0.9937\n",
      "Epoch 871/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0283 - accuracy: 0.9937\n",
      "Epoch 872/1500\n",
      "160/160 [==============================] - 0s 163us/step - loss: 0.0285 - accuracy: 0.9937\n",
      "Epoch 873/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0285 - accuracy: 0.9937\n",
      "Epoch 874/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0282 - accuracy: 0.9937\n",
      "Epoch 875/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0283 - accuracy: 0.9937\n",
      "Epoch 876/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0291 - accuracy: 0.9875\n",
      "Epoch 877/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0279 - accuracy: 0.9937\n",
      "Epoch 878/1500\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.0271 - accuracy: 0.9937\n",
      "Epoch 879/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0276 - accuracy: 0.9937\n",
      "Epoch 880/1500\n",
      "160/160 [==============================] - 0s 168us/step - loss: 0.0273 - accuracy: 0.9937\n",
      "Epoch 881/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0274 - accuracy: 0.9937\n",
      "Epoch 882/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0269 - accuracy: 0.9937\n",
      "Epoch 883/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 884/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0271 - accuracy: 0.9937\n",
      "Epoch 885/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0269 - accuracy: 0.9937\n",
      "Epoch 886/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0270 - accuracy: 0.9937\n",
      "Epoch 887/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0265 - accuracy: 0.9937\n",
      "Epoch 888/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0268 - accuracy: 0.9937\n",
      "Epoch 889/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0262 - accuracy: 0.9937\n",
      "Epoch 890/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0269 - accuracy: 0.9937\n",
      "Epoch 891/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0259 - accuracy: 0.9937\n",
      "Epoch 892/1500\n",
      "160/160 [==============================] - 0s 221us/step - loss: 0.0266 - accuracy: 0.9937\n",
      "Epoch 893/1500\n",
      "160/160 [==============================] - 0s 275us/step - loss: 0.0271 - accuracy: 0.9937\n",
      "Epoch 894/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0256 - accuracy: 0.9937\n",
      "Epoch 895/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0255 - accuracy: 0.9937\n",
      "Epoch 896/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0261 - accuracy: 0.9937\n",
      "Epoch 897/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0250 - accuracy: 0.9937\n",
      "Epoch 898/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0265 - accuracy: 0.9937\n",
      "Epoch 899/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0276 - accuracy: 0.9937\n",
      "Epoch 900/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0255 - accuracy: 0.9937\n",
      "Epoch 901/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0257 - accuracy: 0.9937\n",
      "Epoch 902/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0255 - accuracy: 0.9937\n",
      "Epoch 903/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0256 - accuracy: 0.9937\n",
      "Epoch 904/1500\n",
      "160/160 [==============================] - 0s 231us/step - loss: 0.0250 - accuracy: 0.9937\n",
      "Epoch 905/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 906/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0247 - accuracy: 0.9937\n",
      "Epoch 907/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0246 - accuracy: 0.9937\n",
      "Epoch 908/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0254 - accuracy: 0.9937\n",
      "Epoch 909/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 910/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0244 - accuracy: 0.9937\n",
      "Epoch 911/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0241 - accuracy: 0.9937\n",
      "Epoch 912/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0241 - accuracy: 0.9937\n",
      "Epoch 913/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0235 - accuracy: 0.9937\n",
      "Epoch 914/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 915/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0241 - accuracy: 0.9937\n",
      "Epoch 916/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 917/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0236 - accuracy: 0.9937\n",
      "Epoch 918/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0233 - accuracy: 0.9937\n",
      "Epoch 919/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0240 - accuracy: 0.9937\n",
      "Epoch 920/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0231 - accuracy: 0.9937\n",
      "Epoch 921/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0237 - accuracy: 0.9937\n",
      "Epoch 922/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0230 - accuracy: 0.9937\n",
      "Epoch 923/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 924/1500\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.0243 - accuracy: 0.9937\n",
      "Epoch 925/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0238 - accuracy: 1.0000\n",
      "Epoch 926/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 927/1500\n",
      "160/160 [==============================] - 0s 235us/step - loss: 0.0238 - accuracy: 0.9937\n",
      "Epoch 928/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0225 - accuracy: 0.9937\n",
      "Epoch 929/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 930/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0229 - accuracy: 1.0000\n",
      "Epoch 931/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0230 - accuracy: 0.9937\n",
      "Epoch 932/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0228 - accuracy: 0.9937\n",
      "Epoch 933/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 934/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 935/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0237 - accuracy: 0.9937\n",
      "Epoch 936/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0215 - accuracy: 0.9937\n",
      "Epoch 937/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 938/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 939/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 940/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0235 - accuracy: 0.9937\n",
      "Epoch 941/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.0231 - accuracy: 0.9937\n",
      "Epoch 942/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 943/1500\n",
      "160/160 [==============================] - 0s 168us/step - loss: 0.0210 - accuracy: 0.9937\n",
      "Epoch 944/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 945/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 946/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0212 - accuracy: 0.9937\n",
      "Epoch 947/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0209 - accuracy: 0.9937\n",
      "Epoch 948/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0217 - accuracy: 1.0000\n",
      "Epoch 949/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0227 - accuracy: 0.9937\n",
      "Epoch 950/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0208 - accuracy: 0.9937\n",
      "Epoch 951/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 952/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 953/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 954/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 955/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 956/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0205 - accuracy: 0.9937\n",
      "Epoch 957/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 958/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 959/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 960/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 961/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 962/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 963/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 964/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 965/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 966/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 967/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0194 - accuracy: 1.0000\n",
      "Epoch 968/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 969/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 970/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 971/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 972/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 973/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 974/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 975/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0190 - accuracy: 0.9937\n",
      "Epoch 976/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 977/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 978/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 979/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0189 - accuracy: 1.0000\n",
      "Epoch 980/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0206 - accuracy: 0.9937\n",
      "Epoch 981/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0181 - accuracy: 0.9937\n",
      "Epoch 982/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 983/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 984/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 985/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 986/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 987/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 988/1500\n",
      "160/160 [==============================] - 0s 243us/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 989/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 990/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 991/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 992/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 993/1500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 994/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0184 - accuracy: 0.9937\n",
      "Epoch 995/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 996/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 997/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 998/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 999/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0175 - accuracy: 0.9937\n",
      "Epoch 1000/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 1001/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 1002/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 1003/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 1004/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 1005/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.0168 - accuracy: 1.0000\n",
      "Epoch 1006/1500\n",
      "160/160 [==============================] - 0s 283us/step - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 1007/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 1008/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 1009/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 1010/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 1011/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 1012/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 1013/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 1014/1500\n",
      "160/160 [==============================] - 0s 209us/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 1015/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 1016/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 1017/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 1018/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 1019/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 1020/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 1021/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 1022/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 1023/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 1024/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 1025/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 1026/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 1027/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0149 - accuracy: 1.0000\n",
      "Epoch 1028/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 1029/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 1030/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 1031/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 1032/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 1033/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 1034/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 1035/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 1036/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 1037/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 1038/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 1039/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 1040/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 1041/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 1042/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 1043/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 1044/1500\n",
      "160/160 [==============================] - 0s 260us/step - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 1045/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 1046/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 1047/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 1048/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 1049/1500\n",
      "160/160 [==============================] - 0s 205us/step - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 1050/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 1051/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 1052/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 1053/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 1054/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 1055/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 1056/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 1057/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 1058/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 1059/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 1060/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 1061/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 1062/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 1063/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 1064/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 1065/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 1066/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 1067/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 1068/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 1069/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 1070/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 1071/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 1072/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 1073/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 1074/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 1075/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0124 - accuracy: 1.0000\n",
      "Epoch 1076/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 1077/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 1078/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 1079/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 1080/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 1081/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 1082/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 1083/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 1084/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 1085/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 1086/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 1087/1500\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 1088/1500\n",
      "160/160 [==============================] - 0s 203us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 1089/1500\n",
      "160/160 [==============================] - 0s 167us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 1090/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 1091/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 1092/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 1093/1500\n",
      "160/160 [==============================] - 0s 165us/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 1094/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 1095/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 1096/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 1097/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 1098/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 1099/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 1100/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 1101/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 1102/1500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 1103/1500\n",
      "160/160 [==============================] - 0s 164us/step - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 1104/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 1105/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 1106/1500\n",
      "160/160 [==============================] - 0s 162us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 1107/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 1108/1500\n",
      "160/160 [==============================] - 0s 163us/step - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 1109/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 1110/1500\n",
      "160/160 [==============================] - 0s 156us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 1111/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 1112/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 1113/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 1114/1500\n",
      "160/160 [==============================] - 0s 168us/step - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 1115/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 1116/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 1117/1500\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 1118/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 1119/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 1120/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 1121/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 1122/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 1123/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 1124/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 1125/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 1126/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 1127/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 1128/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 1129/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 1130/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 1131/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 1132/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 1133/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 1134/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 1135/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 1136/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 1137/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 1138/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 1139/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 1140/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 1141/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 1142/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 1143/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 1144/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 1145/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 1146/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 1147/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 1148/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 1149/1500\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 1150/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 1151/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 1152/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 1153/1500\n",
      "160/160 [==============================] - 0s 201us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 1154/1500\n",
      "160/160 [==============================] - 0s 213us/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 1155/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 1156/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 1157/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 1158/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 1159/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 1160/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 1161/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 1162/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 1163/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 1164/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 1165/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 1166/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 1167/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 1168/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 1169/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 1170/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 1171/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 1172/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 1173/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 1174/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 1175/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 1176/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 1177/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 1178/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 1179/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 1180/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 1181/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 1182/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 1183/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 1184/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 1185/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 1186/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 1187/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 1188/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 1189/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 1190/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 1191/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 1192/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 1193/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 1194/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 1195/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 1196/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 1197/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 1198/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 1199/1500\n",
      "160/160 [==============================] - 0s 168us/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 1200/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 1201/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 1202/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 1203/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 1204/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 1205/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 1206/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 1207/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 1208/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 1209/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 1210/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 1211/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 1212/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 1213/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 1214/1500\n",
      "160/160 [==============================] - 0s 219us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 1215/1500\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 1216/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 1217/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 1218/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 1219/1500\n",
      "160/160 [==============================] - 0s 160us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 1220/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 1221/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 1222/1500\n",
      "160/160 [==============================] - 0s 230us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 1223/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 1224/1500\n",
      "160/160 [==============================] - 0s 259us/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 1225/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1226/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 1227/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 1228/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 1229/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1230/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 1231/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1232/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1233/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 1234/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1235/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1236/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1237/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 1238/1500\n",
      "160/160 [==============================] - 0s 218us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 1239/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 1240/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 1241/1500\n",
      "160/160 [==============================] - 0s 215us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 1242/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 1243/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 1244/1500\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 1245/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 1246/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 1247/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 1248/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 1249/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 1250/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 1251/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 1252/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 1253/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 1254/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 1255/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 1256/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 1257/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 1258/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 1259/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 1260/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 1261/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 1262/1500\n",
      "160/160 [==============================] - 0s 220us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 1263/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 1264/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 1265/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 1266/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 1267/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 1268/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 1269/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 1270/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 1271/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 1272/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 1273/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 1274/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 1275/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 1276/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 1277/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 1278/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 1279/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 1280/1500\n",
      "160/160 [==============================] - 0s 171us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 1281/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 1282/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 1283/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 1284/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 1285/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 1286/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 1287/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 1288/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 1289/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 1290/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 1291/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 1292/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 1293/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 1294/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 1295/1500\n",
      "160/160 [==============================] - 0s 212us/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 1296/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 1297/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 1298/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 1299/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 1300/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 1301/1500\n",
      "160/160 [==============================] - 0s 240us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 1302/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 1303/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 1304/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 1305/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 1306/1500\n",
      "160/160 [==============================] - 0s 214us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 1307/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 1308/1500\n",
      "160/160 [==============================] - 0s 207us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 1309/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 1310/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 1311/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 1312/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 1313/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 1314/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 1315/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 1316/1500\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 1317/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 1318/1500\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 1319/1500\n",
      "160/160 [==============================] - 0s 165us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 1320/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 1321/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 1322/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 1323/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 1324/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 1325/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 1326/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 1327/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 1328/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 1329/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 1330/1500\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 1331/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 1332/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 1333/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 1334/1500\n",
      "160/160 [==============================] - 0s 274us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 1335/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 1336/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 1337/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 1338/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 1339/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 1340/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 1341/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 1342/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 1343/1500\n",
      "160/160 [==============================] - 0s 165us/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 1344/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 1345/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 1346/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 1347/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 1348/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 1349/1500\n",
      "160/160 [==============================] - 0s 200us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 1350/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 1351/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 1352/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 1353/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 1354/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 1355/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 1356/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 1357/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 1358/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 1359/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 1360/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 1361/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 1362/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 1363/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 1364/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 1365/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 1366/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 1367/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 1368/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 1369/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1370/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1371/1500\n",
      "160/160 [==============================] - 0s 204us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1372/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1373/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1374/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 1375/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 1376/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1377/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 1378/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 1379/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1380/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 1381/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1382/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 1383/1500\n",
      "160/160 [==============================] - 0s 206us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 1384/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1385/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1386/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1387/1500\n",
      "160/160 [==============================] - 0s 196us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1388/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1389/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1390/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1391/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 1392/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1393/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 1394/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 1395/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1396/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1397/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1398/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 1399/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 1400/1500\n",
      "160/160 [==============================] - 0s 192us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1401/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1402/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1403/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1404/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1405/1500\n",
      "160/160 [==============================] - 0s 217us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 1406/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1407/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 1408/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1409/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 1410/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 1411/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 1412/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1413/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 1414/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 1415/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1416/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 1417/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1418/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 1419/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1420/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1421/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1422/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1423/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1424/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1425/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1426/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1427/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1428/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1429/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1430/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 1431/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1432/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1433/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1434/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1435/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1436/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1437/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1438/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1439/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1440/1500\n",
      "160/160 [==============================] - 0s 193us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1441/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1442/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1443/1500\n",
      "160/160 [==============================] - 0s 199us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1444/1500\n",
      "160/160 [==============================] - 0s 255us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 1445/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1446/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 1447/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1448/1500\n",
      "160/160 [==============================] - 0s 229us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1449/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1450/1500\n",
      "160/160 [==============================] - 0s 208us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1451/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1452/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1453/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1454/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1455/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1456/1500\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1457/1500\n",
      "160/160 [==============================] - 0s 188us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1458/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1459/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1460/1500\n",
      "160/160 [==============================] - 0s 169us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1461/1500\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1462/1500\n",
      "160/160 [==============================] - 0s 202us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1463/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1464/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1465/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1466/1500\n",
      "160/160 [==============================] - 0s 179us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1467/1500\n",
      "160/160 [==============================] - 0s 174us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1468/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1469/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1470/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1471/1500\n",
      "160/160 [==============================] - 0s 190us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1472/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1473/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1474/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1475/1500\n",
      "160/160 [==============================] - 0s 176us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1476/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 1477/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 1478/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1479/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1480/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1481/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1482/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1483/1500\n",
      "160/160 [==============================] - 0s 185us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1484/1500\n",
      "160/160 [==============================] - 0s 173us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1485/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1486/1500\n",
      "160/160 [==============================] - 0s 186us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1487/1500\n",
      "160/160 [==============================] - 0s 180us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1488/1500\n",
      "160/160 [==============================] - 0s 183us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1489/1500\n",
      "160/160 [==============================] - 0s 195us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1490/1500\n",
      "160/160 [==============================] - 0s 182us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1491/1500\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1492/1500\n",
      "160/160 [==============================] - 0s 184us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 1493/1500\n",
      "160/160 [==============================] - 0s 178us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1494/1500\n",
      "160/160 [==============================] - 0s 197us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 1495/1500\n",
      "160/160 [==============================] - 0s 181us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1496/1500\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1497/1500\n",
      "160/160 [==============================] - 0s 191us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 1498/1500\n",
      "160/160 [==============================] - 0s 198us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 1499/1500\n",
      "160/160 [==============================] - 0s 177us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 1500/1500\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0013 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f5ec04c6290>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the final model for evaluating the test dataset\n",
    "print('Forming the final model using: optimizer=%s, kernel=%s, epochs=%d, batch_size=%d'\n",
    "      % (best_optimizer, best_kernel_init, best_epoch, best_batch))\n",
    "final_model = create_customized_model(best_optimizer, best_kernel_init)\n",
    "final_model.fit(X_train, y_train, epochs=best_epoch, batch_size=best_batch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 24)                240       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 6)                 78        \n",
      "=================================================================\n",
      "Total params: 618\n",
      "Trainable params: 618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the final model\n",
    "print(final_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'sequential_7', 'layers': [{'class_name': 'Dense', 'config': {'name': 'dense_19', 'trainable': True, 'batch_input_shape': (None, 9), 'dtype': 'float32', 'units': 24, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'RandomNormal', 'config': {'mean': 0.0, 'stddev': 0.05, 'seed': 894}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_20', 'trainable': True, 'dtype': 'float32', 'units': 12, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'RandomNormal', 'config': {'mean': 0.0, 'stddev': 0.05, 'seed': 894}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}, {'class_name': 'Dense', 'config': {'name': 'dense_21', 'trainable': True, 'dtype': 'float32', 'units': 6, 'activation': 'softmax', 'use_bias': True, 'kernel_initializer': {'class_name': 'RandomNormal', 'config': {'mean': 0.0, 'stddev': 0.05, 'seed': 894}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}}]}\n"
     ]
    }
   ],
   "source": [
    "# Display the configuration of the final model\n",
    "print(final_model.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_keras_scratch_graph_369359 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "54/54 [==============================] - 0s 1ms/step\n",
      "\n",
      "accuracy: 68.52%\n",
      "\n",
      "loss: 5.52\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Keras model on previously unseen data\n",
    "scores = final_model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (final_model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f\" % (final_model.metrics_names[0], scores[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 4 Optimize Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Finalize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 5 Finalize Model has begun! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing op __inference_keras_scratch_graph_369396 in device /job:localhost/replica:0/task:0/device:CPU:0\n",
      "Data item #0 predicted to be [2] (expected [1])\n",
      "Data item #1 predicted to be [1] (expected [1])\n",
      "Data item #2 predicted to be [1] (expected [1])\n",
      "Data item #3 predicted to be [2] (expected [1])\n",
      "Data item #4 predicted to be [3] (expected [6])\n",
      "Data item #5 predicted to be [2] (expected [1])\n",
      "Data item #6 predicted to be [2] (expected [2])\n",
      "Data item #7 predicted to be [7] (expected [7])\n",
      "Data item #8 predicted to be [1] (expected [1])\n",
      "Data item #9 predicted to be [1] (expected [2])\n",
      "Data item #10 predicted to be [2] (expected [2])\n",
      "Data item #11 predicted to be [2] (expected [7])\n",
      "Data item #12 predicted to be [1] (expected [1])\n",
      "Data item #13 predicted to be [2] (expected [2])\n",
      "Data item #14 predicted to be [3] (expected [1])\n",
      "Data item #15 predicted to be [2] (expected [2])\n",
      "Data item #16 predicted to be [2] (expected [2])\n",
      "Data item #17 predicted to be [5] (expected [5])\n",
      "Data item #18 predicted to be [1] (expected [1])\n",
      "Data item #19 predicted to be [1] (expected [1])\n"
     ]
    }
   ],
   "source": [
    "# Make class predictions with the model\n",
    "predictions = final_model.predict_classes(X_test)\n",
    "\n",
    "# Summarize the first 20 cases\n",
    "for i in range(20):\n",
    "\tprint('Data item #%d predicted to be %s (expected %s)' % (i, encoder.inverse_transform([predictions[i]]), encoder.inverse_transform([np.argmax(y_test[i])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (notifyStatus): email_notify(\"Phase 5 Finalize Model completed! \"+datetime.now().strftime('%a %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for the script: 0:19:43.135812\n"
     ]
    }
   ],
   "source": [
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
